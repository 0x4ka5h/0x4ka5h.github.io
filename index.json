[{"categories":null,"content":"Still learning weights ^_~ come again and check for checkpoints!\n","date":"Feb 3, 2022","img":"","permalink":"https://g00g1y5p4.github.io/about/","series":null,"tags":null,"title":"About"},{"categories":[],"content":"Introduction I knew you guys can\u0026rsquo;t bear this much long gap for my presence with our new blogs. That\u0026rsquo;s why this time I came up with a real-time developed project which is submitted to the Government of India.\nThanks for reading! ","date":"Dec 31, 2021","img":"https://g00g1y5p4.github.io/","permalink":"https://g00g1y5p4.github.io/posts/depthestimationofapothole/","series":[],"tags":[],"title":"Depth Estimation of a Pothole on Roads"},{"categories":["FPGA"],"content":"Introduction Have you seen my previous post? If yes, I know you will definitely come here to see my new post.. üòå. what if your answer is No? Then you will definitely come here next timeü•≤.\nWhat\u0026rsquo;s new this timeüôÑ?\u0026hellip; Vehicle Accident detection using FPGA üòÆ‚Äçüí®.\nEw? FPGAüò¶? What\u0026rsquo;s that? Field Programmable Gate Arrays (FPGAs) are semiconductor devices that are based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. FPGAs can be reprogrammed to desired application or functionality requirements after manufacturing.FPGAs contain an array of programmable logic blocks, and a hierarchy of reconfigurable interconnects allowing blocks to be wired together.An FPGA can be used to solve any problem which is computable. This is trivially proven by the fact that FPGAs can be used to implement a soft microprocessor.\nI think its not just simple as I said. It\u0026rsquo;s easy to understand and Hard to learnüòâ.\nWhat actually we did in this project? We had used the DE10Nano FPGA board to program. DE10Nano FPGA Board has 2 different parts:\n  FPGA\n  HPS \u0026mdash; Hard Processor System with a wealth of peripherals onboard for creating some interesting applications\n  The security of the vehicles can be achieved by using two methods on road and the other one is off road. On road means providing security from the accidents..So We are decided to Save lives from accidents. We are using ADXL sensor means Accelometer aka GyroScope, Magentometer to determine the vehicle position on the ground.\nHave you think that this idea is already developed in CHITRALAHARI(Telugu) movie üòíü§´? Whatever, but we dont take this idea from any movie. No means noüôÑ, we dont take it from thereü•≤, Haha\u0026hellip;\nLets dive into the project development and let me explain the basics code for this system. It has developed using python, C, Embedded-C, Java the development of android application has done using eclipse and this overall implementation can run based on IOT.\nHuh.. number of languaguesü§î..Do we really need 4-5 languages or am I making some show off üò¢? Ofcourse, Yes. YesüôÑ? show offüòÖ? No. Not for show off. In HPS, If we want to access accelometer we have to access via protocols using some code. So thats why I\u0026rsquo;m using C language to access.\nIncluding required libraries and Define paths for i2c protocol event file to access ADXL sensor. #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/stat.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;error.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;errno.h\u0026gt;#include \u0026lt;limits.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026#34;linux/input.h\u0026#34; #define INPUT_DEV_NODE \u0026#34;/dev/input/by-path/platform-ffc04000.i2c-event\u0026#34; #define SYSFS_DEVICE_DIR \u0026#34;/sys/devices/platform/soc/ffc04000.i2c/i2c-0/0-0053/\u0026#34;  #define EV_CODE_X (0) #define EV_CODE_Y (1) #define EV_CODE_Z (2)  #define LOOP_COUNT (1000) Let me define a function to overwrite the files data to access ADXL Initially the ADXL sensor is disabled. So we need to overwrite the data and enable the ADXL.\nvoid write_sysfs_cntl_file(const char *dir_name, const char *file_name, const char *write_str) { char path[PATH_MAX]; int path_length; int file_fd; int result; // create the path to the file we need to open \tpath_length = snprintf(path, PATH_MAX, \u0026#34;%s/%s\u0026#34;,\tdir_name, file_name); if(path_length \u0026lt; 0) error(1, 0, \u0026#34;path output error\u0026#34;); if(path_length \u0026gt;= PATH_MAX) error(1, 0, \u0026#34;path length overflow\u0026#34;); // open the file \tfile_fd = open(path, O_WRONLY | O_SYNC); if(file_fd \u0026lt; 0) error(1, errno, \u0026#34;could not open file \u0026#39;%s\u0026#39;\u0026#34;, path); // write the string to the file \tresult = write(file_fd, write_str, strlen(write_str)); if(result \u0026lt; 0) error(1, errno, \u0026#34;writing to \u0026#39;%s\u0026#39;\u0026#34;, path); if((size_t)(result) != strlen(write_str)) error(1, errno, \u0026#34;buffer underflow writing \u0026#39;%s\u0026#39;\u0026#34;, path); // close the file \tresult = close(file_fd); if(result \u0026lt; 0) error(1, errno, \u0026#34;could not close file \u0026#39;%s\u0026#39;\u0026#34;, path); } Enable and Access ADXL sensor We can enable adxl sensor in de10nano by writing \u0026ldquo;0\u0026rdquo; in /sys/devices/platform/soc/ffc04000.i2c/i2c-0/0-0053/disable file\n// enable adxl write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;disable\u0026#34;, \u0026#34;0\u0026#34;); // set the sample rate to maximum write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;rate\u0026#34;, \u0026#34;15\u0026#34;); // do not auto sleep write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;autosleep\u0026#34;, \u0026#34;0\u0026#34;); // open the event device node event_dev_fd = open(input_dev_node, O_RDONLY | O_SYNC); So we enabled the ADXL sensor and already opened the device to get input values from ADXL sensor.\nif(event_dev_fd \u0026lt; 0) error(1, errno, \u0026#34;could not open file \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); // read the current state of each axis \tprintf(\u0026#34;\\n\u0026#34;); for(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); } fflush(stdout); result = read(event_dev_fd, \u0026amp;the_event, sizeof(struct input_event)); if(result \u0026lt; 0) error(1, errno, \u0026#34;reading %d from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); if(result != sizeof(struct input_event)) error(1, 0, \u0026#34;did not read %d bytes from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); // read the current state of each axis \tfor(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); abs_value_array[i] = the_absinfo.value; } printf(\u0026#34;%d %d %d\u0026#34;,abs_value_array[0],abs_value_array[1],abs_value_array[2]) Let me explain the above code, otherwise its looks like I copied from some sourceü§ß.\nI forgot to say that What is ADXL sensor and what it will return as output? Yeah, yeah We use ADXL sensor aka GyroScope in mobile phones also. I think that pubG also uses the Gyroscope and detect if players got accidentüòÇ. I am right amn\u0026rsquo;t Iü•±?\nJokes apart. GryoScope is a device used for measuring or maintaining orientation and angular velocity. It is a spinning wheel or disc in which the axis of rotation (spin axis) is free to assume any orientation by itself. So if we want to determine a position in a 3rd space we actually want x,y,z coordinates. Like this way ADXL sensor also return x,y,z coordinates of a device. Those x,y,z coordinates are printed at last of the above code which are taken from device\u0026rsquo;s triggered event.\nSo\u0026hellip;! So what? I think now you guys also do this project. Whatü§Ø? Did you detect any accident? No. Then How can you finish this blog here?\u0026hellip; What you guys are thinking is right, but I have explained you everything. Looks like I\u0026rsquo;m crazy, amn\u0026rsquo;t I? Ofcourse, may be I\u0026rsquo;müòâ.\nOk Let me explain in details how I finised, Dont worry..üòå!\nAccident detection using ADXL Sensor values If you run a loop infinitely, what happens? what happens\u0026hellip; system got stucked..üòÖ. usshü§´ Dont make jokes.. if we run a loop inifnitely, then we can calculate the difference between the values of coordinates in two consecutive seconds crosses threshhold of a normal vehicle position, then I considered it as an accident. But how? how means \u0026hellip; the sudden changes in coordinates represent sudden movement compare to its normal speed. So If a sudden change in coordinate then I thought some collision is occured.\nint main(void) { int event_dev_fd; const char *input_dev_node = INPUT_DEV_NODE; int result; int i; int loop; struct input_event the_event; struct input_absinfo the_absinfo; int abs_value_array[3] = {0}; int last_value_x=0; int last_value_y=0; int last_value_z=0; int x_abs_value_array=0; int y_abs_value_array=0; int z_abs_value_array=0; int check=1; system(\u0026#34;chmod 777 /sys/class/gpio/export\u0026#34;); system(\u0026#34;echo 107 \u0026gt; /sys/class/gpio/export\u0026#34;); system(\u0026#34;echo out \u0026gt; /sys/class/gpio/gpio107/direction\u0026#34;); system(\u0026#34;echo 1 \u0026gt; /sys/class/gpio/gpio107/value\u0026#34;); // enable adxl \twrite_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;disable\u0026#34;, \u0026#34;0\u0026#34;); // set the sample rate to maximum \twrite_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;rate\u0026#34;, \u0026#34;15\u0026#34;); // do not auto sleep \twrite_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;autosleep\u0026#34;, \u0026#34;0\u0026#34;); // open the event device node \tevent_dev_fd = open(input_dev_node, O_RDONLY | O_SYNC); if(event_dev_fd \u0026lt; 0) error(1, errno, \u0026#34;could not open file \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); // read the current state of each axis \tprintf(\u0026#34;\\n\u0026#34;); for(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); } fflush(stdout); // read the next LOOP_COUNT events \tfor(loop = 0 ;; loop++) { // read the next event \tresult = read(event_dev_fd, \u0026amp;the_event, sizeof(struct input_event)); if(result \u0026lt; 0) error(1, errno, \u0026#34;reading %d from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); if(result != sizeof(struct input_event)) error(1, 0, \u0026#34;did not read %d bytes from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); // read the current state of each axis \tfor(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); abs_value_array[i] = the_absinfo.value; } if (last_value_x!=0 || last_value_y!=0 || last_value_y!=0){ if (((abs(abs_value_array[0]-last_value_x))\u0026gt;200 || (abs(abs_value_array[1]-last_value_y))\u0026gt;200) \u0026amp;\u0026amp; ((abs(abs_value_array[0]-last_value_x))\u0026gt;200 || (abs(abs_value_array[2]-last_value_z))\u0026gt;200) \u0026amp;\u0026amp; ((abs(abs_value_array[1]-last_value_y))\u0026gt;200 || (abs(abs_value_array[2]-last_value_z))\u0026gt;200)){ printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[0]-last_value_x)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[1]-last_value_y)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[2]-last_value_z)); printf(\u0026#34; -- %d,%d,%d ----- \\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2] ); system(\u0026#34;echo 0 \u0026gt; /sys/class/gpio/gpio107/value\u0026#34;); printf(\u0026#34; ------------ accident occurs ----------\\n\u0026#34;); break; } if((abs(abs_value_array[0]-last_value_x))\u0026gt;300 || (abs(abs_value_array[1]-last_value_y))\u0026gt;300 || (abs(abs_value_array[2]-last_value_z))\u0026gt;300){ printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[0]-last_value_x)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[1]-last_value_y)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[2]-last_value_z)); printf(\u0026#34; ------------- %d,%d,%d ----------------- \\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2] ); system(\u0026#34;echo 0 \u0026gt; /sys/class/gpio/gpio107/value\u0026#34;); printf(\u0026#34; ------------ accident occurs ----------\\n\u0026#34;); break; } } if (check%50==0){ last_value_x = abs_value_array[0]; last_value_y = abs_value_array[1]; last_value_z = abs_value_array[2]; printf(\u0026#34;%d,%d,%d ----------------- \\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2] ); check++; } if (x_abs_value_array!=abs_value_array[0] \u0026amp;\u0026amp; y_abs_value_array!=abs_value_array[1] \u0026amp;\u0026amp; z_abs_value_array!=abs_value_array[2] ){ check++; printf(\u0026#34;%d,%d,%d --- %d\\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2], check); } x_abs_value_array = abs_value_array[0]; y_abs_value_array = abs_value_array[1]; z_abs_value_array = abs_value_array[2]; } result = close(event_dev_fd); if(result \u0026lt; 0) error(1, errno, \u0026#34;could not close file \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); // disable adxl \twrite_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;disable\u0026#34;, \u0026#34;1\u0026#34;); } watch.c In the above code, I power up the 107th gpio pin in DE10Nano when collision occured. Yooo We did it..! Whatüòµ? .. You just did accident detection, but how others can notice that accident has been happend to thier vehicle? Author is irritating broüòñ huhh?\nOooov oov O.. I just forget. Dont tease me broüòï,I will explain. NodeMCU I have connected the 107th gpio output wire to NodeMCU to take input. So when the 107th gpio pin is powered up, then the NodeMCU read the it and sends the data to CLOUD.\n#include \u0026lt;ESP8266WiFi.h\u0026gt;#include \u0026lt;WiFiClient.h\u0026gt;#include \u0026lt;ESP8266WebServer.h\u0026gt; #include \u0026#34;Adafruit_MQTT.h\u0026#34;#include \u0026#34;Adafruit_MQTT_Client.h\u0026#34; #define AIO_SERVER \u0026#34;io.adafruit.com\u0026#34; #define AIO_SERVERPORT 1883 #define AIO_USERNAME \u0026#34;g00g1y5p4\u0026#34; #define AIO_KEY \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; // Obtained from account info on io.adafruit.com int i=0; int s=0; int k=0; int n=0; /* Set these to your desired credentials. */ const char *ssid = \u0026#34;SSID\u0026#34;; //Enter your WIFI ss const char *password = \u0026#34;PASSWD\u0026#34;; //Enter your WIFI password  ESP8266WebServer server(80); void handleRoot() { server.send(200, \u0026#34;text/html\u0026#34;, \u0026#34;\u0026#34;); } void handleSave() { if (server.arg(\u0026#34;pass\u0026#34;) != \u0026#34;\u0026#34;) { Serial.println(server.arg(\u0026#34;pass\u0026#34;)); } } WiFiClient client; // Setup the MQTT client class by passing in the WiFi client and MQTT server and login details. Adafruit_MQTT_Client mqtt(\u0026amp;client, AIO_SERVER, AIO_SERVERPORT, AIO_USERNAME, AIO_KEY); Adafruit_MQTT_Publish Attendance = Adafruit_MQTT_Publish(\u0026amp;mqtt, AIO_USERNAME \u0026#34;/feeds/iot\u0026#34;); void setup() { pinMode(LED_BUILTIN, OUTPUT); delay(3000); Serial.begin(115200); pinMode(0, INPUT_PULLUP); WiFi.begin(ssid, password); while (WiFi.status() != WL_CONNECTED) { delay(500); Serial.print(\u0026#34;.\u0026#34;); } pinMode(4, INPUT); server.on(\u0026#34;/Python\u0026#34;, handleRoot); server.on (\u0026#34;/save\u0026#34;, handleSave); server.begin(); connect(); } void connect() { Serial.print(F(\u0026#34;Connecting to Adafruit IO... \u0026#34;)); int8_t ret; while ((ret = mqtt.connect()) != 0) { switch (ret) { case 1: Serial.println(F(\u0026#34;Wrong protocol\u0026#34;)); break; case 2: Serial.println(F(\u0026#34;ID rejected\u0026#34;)); break; case 3: Serial.println(F(\u0026#34;Server unavail\u0026#34;)); break; case 4: Serial.println(F(\u0026#34;Bad user/pass\u0026#34;)); break; case 5: Serial.println(F(\u0026#34;Not authed\u0026#34;)); break; case 6: Serial.println(F(\u0026#34;Failed to subscribe\u0026#34;)); break; default: Serial.println(F(\u0026#34;Connection failed\u0026#34;)); break; } if(ret \u0026gt;= 0) mqtt.disconnect(); Serial.println(F(\u0026#34;Retrying connection...\u0026#34;)); delay(5000); } Serial.println(F(\u0026#34;Adafruit IO Connected!\u0026#34;)); if (! Attendance.publish(\u0026#34;g00g1y\u0026#34;)){ Serial.println(F(\u0026#34;Failed\u0026#34;)); } else { Serial.println(F(\u0026#34;Sent!\u0026#34;)); } } void loop() { server.handleClient(); if (digitalRead(0)== HIGH){ i=0; Serial.println(\u0026#34;LOW\u0026#34;); }else if(digitalRead(0)==LOW \u0026amp;\u0026amp; i==0 ){ k=0; if (s==0){ if (! Attendance.publish(\u0026#34;reset\u0026#34;)) { //Publish to Adafrui  Serial.println(F(\u0026#34;Failed\u0026#34;)); } else { i+=1; Serial.println(F(\u0026#34;Sent!\u0026#34;)); } } } delay(1000); if (analogRead(4)\u0026gt;=250 \u0026amp;\u0026amp; k==0){ if (n==0){ n+=1; if (! Attendance.publish(\u0026#34;g00g1yg00g1y\u0026#34;)) { //Publish to Adafrui  Serial.println(F(\u0026#34;Failed\u0026#34;)); } else { k+=1; Serial.println(F(\u0026#34;Sent!\u0026#34;)); } } }else if(analogRead(4)\u0026lt;=200 \u0026amp;\u0026amp; k!=0){ n=0; } } send_alarm.ino And I created an Android Application to request the cloud data and if the accident code is detected then the alarm will play sound continuously through that application even if we terminate the Application\u0026rsquo;s background process.\nIf accident occured\u0026hellip; Led will blink.\nVehicle-Accident-Detection-using-DE10-Nano Hope you guys like and Subscribe to this Utube account üòÇüòÇ.\n Thanks for reading! ","date":"Dec 31, 2021","img":"https://g00g1y5p4.github.io/posts/vehicleaccidentdetectionusingfpga/images/1.jpeg","permalink":"https://g00g1y5p4.github.io/posts/vehicleaccidentdetectionusingfpga/","series":[],"tags":["DE10Nano","NodeMCU"],"title":"Vehicle Accident Detection Using FPGA"},{"categories":["Computer Vision"],"content":"Hi guys,How are you? I hope you guys are fine and missing my blogs so much.. Hah!!\nThis time I coming with a new topic mostly based on Image processing and ComputerVision technologies. Its sounds little bit different than previous blog, isn\u0026rsquo;t it? In this blog, we address a drowsy driver alert system that has been developed using such a technique in which the Video Stream Processing (VSP) is analyzed by eye blink concept through an Eye Aspect Ratio (EAR) and Euclidean distance of the eye. Face landmark algorithm is also used as a proper way to eye detection. When the driver‚Äôs fatigue is detected, the IoT module issues a warning message along with impact of collision and location information, thereby alerting with the help of a voice speaking through the Raspberry Pi monitoring system. Lets dive into the problem and discuss how this can be solve by using Image Processing techniques.First of all what we need to do in this problem is to Detect face and eyes to calculate EAR ratio of eyes. So If a face is found, we apply facial landmark detection and extract the regions.Now that we have the eye regions, so we can calculate EAR to check whether the eyes are closed or not..! Initially I didn\u0026rsquo;t get correct output until many trails and errors.\nLets start .. include required libraries import numpy as np import os,time,sys import cv2 import dlib Hah .. ! You guys may think why cv2, dlib and numpy libraries. cv2 is for computervision, dlib used to plot facial landmarks over Drivers face,numpy is used to manipulate Image arrays. Well okay.. Do we really need time library? Do we need to calculate time or something üòí.. *(-_-)*? Ofcourse, Yes. We need to calculate time difference between blink of an eye. Its sounds intresting, isn\u0026rsquo;t it? But I was irritated first time when I try to develop a algorithm to solve this program.\nRecognize Face and EYEs cap=cv2.VideoCapture(0) detector=dlib.get_frontal_face_detector() predictor=dlib.shape_predictor(\u0026#34;shape_predictor_68_face_landmarks.dat\u0026#34;) _,frame=cap.read() gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) faces=detector(gray) font=cv2.FONT_HERSHEY_SIMPLEX for face in faces: x=face.left() y=face.top() h=face.right() k=face.bottom() #cv2.rectangle(gray,(x,y),(h,k),(0,255,0),1) landmarks=predictor(gray,face) eye1=[] for n in range(36, 42): eye1.append(landmarks.part(n)) x = landmarks.part(n).x y = landmarks.part(n).y cv2.circle(gray, (x, y), 1, (255, 0, 0), 0) eye2=[] for n in range(42, 48): eye2.append(landmarks.part(n)) x = landmarks.part(n).x y = landmarks.part(n).y cv2.circle(gray, (x, y), 1, (255, 0, 0), 0) ü§ß.. What? whats happening on above code ah? Looks like irritating..üòñ, Dont worry I will explain hehe..hah..ü•≤üòÖ.\nLet me explain the above code, loading dlib facial landmarks and read an image frame from camera. Apply pretrained dlib facial landmarks alogrithm on resultant frame. There are total 68 facial landmarks (based on the model we choose).\nIn 68 landmarks, from 36 to 42 denote left eye and from 42 to 48 denotes right eye. Calculate EAR (Eye Aspect Ratio) def calculus(l): x_d=(l[1][0]-l[5][0])**2 y_d=(l[1][1]-l[5][1])**2 p2_p6=np.sqrt(x_d+y_d) x_d=(l[2][0]-l[4][0])**2 y_d=(l[2][1]-l[4][1])**2 p3_p5=np.sqrt(x_d+y_d) x_d=(l[0][0]-l[3][0])**2 y_d=(l[0][1]-l[3][1])**2 p1_p4=np.sqrt(x_d+y_d) ratio=(p2_p6+p3_p5)/p1_p4 return ratio eye=[] for i in eye1: p=str(i) try: x=int(p[1:4].strip()) y=int(p[6:len(p)-1].strip()) except: x=int(p[1:3].strip()) y=int(p[5:len(p)-1].strip()) eye.append([x,y]) ratio1=calclus(eye) eye=[] for i in eye2: p=str(i) try: x=int(p[1:4].strip()) y=int(p[6:len(p)-1].strip()) except: x=int(p[1:3].strip()) y=int(p[5:len(p)-1].strip()) eye.append([x,y]) ratio2=calclus(eye) EAR=(ratio1+ratio2)/4 Yoooo,Hohooo ü•≥, we calculated the EAR(Eye Aspect Ratio) very finely. Really? How? I don\u0026rsquo;t understand the calculus part.. it looks like bricksblock in a row with some mathematical expression. Simply its calculates the ((p2-p6)+(p3-p6))/(p1-p4) hehe..^_^\nBOOM! we already achieved our final solution. Really? Ofcourse, Yes. we just need a loop to read frames from the camera and calculate EAR difference between two consecutive seconds.So we have to find the time difference and check if the EAR crossed threshhold values then check if the time difference between last blink time and present time crosses 2 seconds, then it means the driver going to sleep. So we need to play a alarm to wake him up.That\u0026rsquo;s it..! Done..ü•≤üòå\nI think its fine and I really dont want to explain more than this üòúüôÉ.\ndetection_.py Hope you guys like this and Subscribe my Utube account üòÇüòÇ.\n Thanks for reading! ","date":"Oct 10, 2020","img":"https://g00g1y5p4.github.io/posts/driverdrowsinessdetection/images/home.png","permalink":"https://g00g1y5p4.github.io/posts/driverdrowsinessdetection/","series":[],"tags":["Face Detection","Image Processing"],"title":"Driver Drowsiness Detection"},{"categories":["GAN's"],"content":"What is GAN\u0026rsquo;s? Generative Adversarial Networks GAN\u0026rsquo;s are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A generator learns to create images that look real, while a discriminator learns to tell real images apart from fakes. Case: I have read that, in most of the cases Doctors may face issues while they are making decisions over CT scanned Images because of Gaussian Noise and making electronic Noise (unwanted disturbance in a signal which is reflected in output). So I tried to implement and train a Deep Learning model using GAN Technology. Brief Solution: Step-1. Preprocessing the Data to train the model.\nStep-2. Generator Model: Used to generate new images which look like real images.\nStep-3. Discriminator Model: Used to classify images as real or fake.\nStep-4. Fitting the model.\nDetailed Solution: We‚Äôll begin by importing needed libraries, considering you‚Äôve installed all the necessary libraries already.\nimport matplotlib.pyplot as plt from IPython.display import display, HTML from PIL import Image,ImageFile import tensorFlow as tf from io import BytesIO import numpy as np import os,sys,math %matplotlib inline Okay,the libraries are succesfully imported and lets go to the next part of this process. Define some filters manually using numpy and image processing techniques to morph the original image.\ndef add_noise(a): a2 = a.copy() rows = a2.shape[0] cols = a2.shape[1] s = 2 # size of spot is 1/20 of smallest dimension for i in range(100): x = np.random.randint(cols-s) y = np.random.randint(rows-s) a2[y:(y+s),x:(x+s)] = 0 return a2 Now we had defined a simple noise funtion which is used to add some random noise pixels (blocks) to the input image \u0026lsquo;a\u0026rsquo;.Let\u0026rsquo;s try to load and pass the image to test above noise function.\nimage = tf.io.read_file(\u0026#39;sample.png\u0026#39;) img = tf.image.decode_png(image , channels=3) img = tf.image.convert_image_dtype(img, tf.float32) img = tf.image.resize(img, [128, 128]) plt.figure() plt.imshow(img) img_array = np.asarray(img) #cols, rows = img.size # Add noise img_array_noise = add_noise(img_array) plt.figure() plt.imshow(img_array_noise) Great, We have successfully implemented the noise function.Lets implement some more funtions which are also behaving like above noise functions but with different different noises.\ndef contrast(img): img=tf.image.random_contrast(img, 0.5, 0.7) return img img_array_noise = contrast(img_array) plt.figure() plt.imshow(img_array_noise) def pepper_noise(img): noise_factor = 0.08 input_image = img + noise_factor * tf.random.normal(img.shape) input_image = tf.squeeze(tf.clip_by_value(input_image, clip_value_min=0., clip_value_max=1.)) return input_image img_array_noise = pepper_noise(img_array) plt.figure() plt.imshow(img_array_noise) def gaussian_noise(image): with tf.name_scope(\u0026#39;Add_gaussian_noise\u0026#39;): noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=(50)/(255), dtype=tf.float32) noise_img = image + noise noise_img = tf.clip_by_value(noise_img, 0.0, 1.0) return noise_img img_array_noise = gaussian_noise(img_array) plt.figure() plt.imshow(img_array_noise) Let\u0026rsquo;s do the same for the whole data set and load them into pickle using numpy arrays for future use.\nimport pickle os.chdir(\u0026#39;/content/drive/MyDrive/project\u0026#39;) if \u0026#39;data.pkl\u0026#39; in os.listdir(): data=1 file = open(\u0026#34;/content/drive/MyDrive/project/data.pkl\u0026#34;, \u0026#34;rb\u0026#34;); x = pickle.load(file) y = pickle.load(file) else: x = np.array([],dtype=tf.float32) y = np.array([],dtype=tf.float32) for path, subdirs, files in os.walk(\u0026#39;path/training_images/\u0026#39;): for name in files: name = os.path.join(path,name) print(name,i) image = tf.io.read_file(name) img = tf.image.decode_png(image , channels=3) img = tf.image.convert_image_dtype(img, tf.float32) img = tf.image.resize(img, [128, 128]) a=0 # Add noise if a%4==0: img_array_noise = add_noise(img_array) elif a%4==1: img_array_noise = pepper_noise(img_array) elif a%4==2: img_array_noise = gaussian_noise(img_array) else: img_array_noise =contrast(img_array) a+=1 x=np.append(x,[img_array],axis=0) y=np.append(y,[img_array_noise],axis=0) x.shape,img_array.shape ((9287, 128, 128, 3), (128, 128, 3))  Lets check the loaded pickle data by loading the pickle.file and seee what comes.\nimport pickle if data != 2: plt.figure() plt.imshow(x[5001]) # Write to file. file = open(\u0026#34;/content/drive/MyDrive/project/data.pkl\u0026#34;, \u0026#34;wb\u0026#34;) pickle.dump(x, file) pickle.dump(y, file) file.close() x_train.shape,x_train_noisy.shape ((9287, 128, 128, 3), (9287, 128, 128, 3))  Yahooo! we are complete preprocessing the data for training GAN\u0026rsquo;s. Source dataPreprocessing.ipynb ##check part-2 for GAN\u0026rsquo;s implementatio Thanks for reading! ","date":"Sep 6, 2020","img":"https://g00g1y5p4.github.io/posts/imagrestorationusinggans_1/images/home.jpg","permalink":"https://g00g1y5p4.github.io/posts/imagrestorationusinggans_1/","series":[],"tags":["Machine Learning","Deep Learning","Image Processing"],"title":"Image Restoration Using GAN's Part-1"},{"categories":["GAN's"],"content":"Lets continue the part-1 ^_^ ;).., We already know that what GAN\u0026rsquo;s are and why they are used. Well, not really. GANs belong to implicit learning methods. In explicit learning models, the model learns its weights directly from the data. whereas in implicit learning, the model learns without the data directly passing through the network. so it is reinforcement learning? Not really Hah..,!\nAs of now,we already discussed that GAN\u0026rsquo;s have two model, and two models are trained simultaneously by an adversarial process. A generator learns to create images that look real, while a discriminator learns to tell real images apart from fakes. See what happens in this contest of discrimination and generation, we may think while the generator is almost discriminated, so it was trained heavely but the thing is the discriminator also trained harldy to discriminate images for small mistakes also.This is why GAN\u0026rsquo;s became intresting in Neural Networks.\nLoad required libraries from tensorflow import keras import tensorflow as tf import numpy as np from tensorflow.keras.preprocessing import image_dataset_from_directory from IPython import display import os,time Loading pickled data into NumPy array Load the Preprocessed Data into NumPy array which is created in Part-1. Pickled data contains preprocessed image data in array format.\nimport pickle # Read from file. file = open(\u0026#34;/../../data.pkl\u0026#34;, \u0026#34;rb\u0026#34;); x_train = pickle.load(file) x_train_noisy = pickle.load(file) file.close() spliting Data into Train and test data The train_test_split function allows you to break a dataset with ease while pursuing an ideal model. Also, keep in mind that your model should not be overfitting or underfitting.\nfrom sklearn.model_selection import train_test_split x_train_, x_test, x_train_noisy_, x_test_noisy = train_test_split(x_train, x_train_noisy,shuffle=128) BUFFER_SIZE = 400 BATCH_SIZE = 64 IMG_WIDTH = 256 IMG_HEIGHT = 256 EPOCHS = 150 OUTPUT_CHANNELS = 3 Implementation of GAN\u0026rsquo;s Upsampling and Downsampling of an Image Downsampling is the process of reducing the sampling rate of a signal. Downsample reduces the sampling rate of the input AOs by an integer factor by picking up one out of N samples. Note that no anti-aliasing filter is applied to the original data.\ndef downsample(filters, size, apply_batchnorm=True): initializer = tf.random_normal_initializer(0., 0.02) result = tf.keras.Sequential() result.add( tf.keras.layers.Conv2D(filters, size, strides=2, padding=\u0026#39;same\u0026#39;, kernel_initializer=initializer, use_bias=False)) if apply_batchnorm: result.add(tf.keras.layers.BatchNormalization()) result.add(tf.keras.layers.LeakyReLU()) return result Upsampling is the process of inserting zero-valued samples between original samples to increase the sampling rate. (This is sometimes called ‚Äúzero-stuffing‚Äù.) This kind of upsampling adds undesired spectral images to the original signal, which are centered on multiples of the original sampling rate\ndef upsample(filters, size, apply_dropout=False): initializer = tf.random_normal_initializer(0., 0.02) result = tf.keras.Sequential() result.add( tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding=\u0026#39;same\u0026#39;, kernel_initializer=initializer, use_bias=False)) result.add(tf.keras.layers.BatchNormalization()) if apply_dropout: result.add(tf.keras.layers.Dropout(0.5)) result.add(tf.keras.layers.ReLU()) return result Loss Functions GANs try to replicate a probability distribution. They should therefore use loss functions that reflect the distance between the distribution of the data generated by the GAN and the distribution of the real data.\nGenerator Loss LAMBDA = 100 loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) def generator_loss(disc_generated_output, gen_output, target): gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output) # mean absolute error l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) total_gen_loss = gan_loss + (LAMBDA * l1_loss) return total_gen_loss, gan_loss, l1_loss Discriminator Loss def discriminator_loss(disc_real_output, disc_generated_output): real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output) generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output) total_disc_loss = real_loss + generated_loss return total_disc_loss The Generator Model The Generator Model generates new images by taking a fixed size random noise as an input. Generated images are then fed to the Discriminator Model.\nThe main goal of the Generator is to fool the Discriminator by generating images that look like real images and thus makes it harder for the Discriminator to classify images as real or fake.\ndef Generator(): inputs = tf.keras.layers.Input((128, 128, 3)) print(1) down_stack = [ downsample(128, 4, apply_batchnorm=False), #downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64) #downsample(128, 4), # (bs, 64, 64, 128) downsample(256, 4), # (bs, 32, 32, 256) downsample(512, 4), # (bs, 16, 16, 512) downsample(512, 4), # (bs, 8, 8, 512) downsample(512, 4), # (bs, 4, 4, 512) downsample(512, 4), # (bs, 2, 2, 512) downsample(512, 4), # (bs, 1, 1, 512) ] up_stack = [ upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024) upsample(512, 4), #apply_dropout=True), # (bs, 4, 4, 1024) upsample(512, 4), #apply_dropout=True), # (bs, 8, 8, 1024) upsample(512, 4), # (bs, 16, 16, 1024) upsample(256, 4), # (bs, 32, 32, 512) upsample(128, 4), # (bs, 64, 64, 256) #upsample(64, 4), # (bs, 128, 128, 128) #upsample(64, 4) ] initializer = tf.random_normal_initializer(0., 0.02) last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding=\u0026#39;same\u0026#39;, kernel_initializer=initializer, activation=\u0026#39;tanh\u0026#39;) # (bs, 256, 256, 3) x = inputs # Downsampling through the model skips = [] for down in down_stack: x = down(x) skips.append(x) skips = reversed(skips[:-1]) # Upsampling and establishing the skip connections for up, skip in zip(up_stack, skips): x = up(x) x = tf.keras.layers.Concatenate()([x, skip]) x = last(x) return tf.keras.Model(inputs=inputs, outputs=x) generator = Generator() tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64) gen_output = generator(x_train_noisy_[0][tf.newaxis, ...], training=False) plt.imshow(gen_output[0, ...]) The Discriminator Model The Discriminator Model takes an image as an input (generated and real) and classifies it as real or fake.\nGenerated images come from the Generator and the real images come from the training data.\nThe discriminator model is the simple binary classification model.\nNow, let us combine both the architectures and understand them in detail.\ndef Discriminator(): initializer = tf.random_normal_initializer(0., 0.02) inp = tf.keras.layers.Input(shape=[128, 128, 3], name=\u0026#39;input_image\u0026#39;) tar = tf.keras.layers.Input(shape=[128, 128, 3], name=\u0026#39;target_image\u0026#39;) x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2) down1 = downsample(64, 4, False)(x) #down1 = downsample(64, 4, False)(down1) # (bs, 128, 128, 64) #down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128) down3 = downsample(256, 4)(down1) # (bs, 32, 32, 256) zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1) # (bs, 31, 31, 512) batchnorm1 = tf.keras.layers.BatchNormalization()(conv) leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1) zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512) last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1) return tf.keras.Model(inputs=[inp, tar], outputs=last) discriminator = Discriminator() tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64) disc_out = discriminator([x_train_noisy_[0][tf.newaxis, ...], gen_output], training=False) plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap=\u0026#39;RdBu_r\u0026#39;) plt.colorbar() Optimizers generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) checkpoints Checkpoints capture the exact value of all parameters ( tf. Variable objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\ncheckpoint_dir = \u0026#39;/content/drive/MyDrive/project/training_checkpoints\u0026#39; checkpoint_prefix = os.path.join(checkpoint_dir, \u0026#34;ckpt\u0026#34;) checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discriminator_optimizer=discriminator_optimizer, generator=generator, discriminator=discriminator) generate images def generate_images(model, test_input, tar): print(111) prediction = model(test_input, training=True) print(111) plt.figure(figsize=(15, 15)) display_list = [test_input[0], tar[0], prediction[0]] title = [\u0026#39;Input Image\u0026#39;, \u0026#39;Ground Truth\u0026#39;, \u0026#39;Predicted Image\u0026#39;] for i in range(3): plt.subplot(1, 3, i+1) plt.title(title[i]) # getting the pixel values between [0, 1] to plot it. plt.imshow(display_list[i] * 0.5 + 0.5) plt.axis(\u0026#39;off\u0026#39;) plt.show() example_input=x_train_noisy_[100:101] example_target=x_train_[100:101] generate_images(generator, example_input, example_target) @tf.function def train_step(input_image, target, epoch): with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: gen_output = generator(input_image, training=True) disc_real_output = discriminator([input_image, target], training=True) disc_generated_output = discriminator([input_image, gen_output], training=True) gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target) disc_loss = discriminator_loss(disc_real_output, disc_generated_output) generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables) discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables)) with summary_writer.as_default(): tf.summary.scalar(\u0026#39;gen_total_loss\u0026#39;, gen_total_loss, step=epoch) tf.summary.scalar(\u0026#39;gen_gan_loss\u0026#39;, gen_gan_loss, step=epoch) tf.summary.scalar(\u0026#39;gen_l1_loss\u0026#39;, gen_l1_loss, step=epoch) tf.summary.scalar(\u0026#39;disc_loss\u0026#39;, disc_loss, step=epoch) lenl=len(x_test) def fitt(x_train_,x_train_noisy_, epochs, x_test,x_test_noisy): k=0 for epoch in range(epochs): start = time.time() display.clear_output(wait=True) #lenl=len(x_test) example_input=x_test_noisy[k:k+1] example_target=x_test[k:k+1] generate_images(generator, example_input, example_target) if k==lenl-2: k=0 k+=1 print(\u0026#34;Epoch: \u0026#34;, epoch) # Train n=0 s=0 for i in range(len(x_train_)): if (n+1) % 100 == 0: print(\u0026#39;.\u0026#39;, end=\u0026#39;\u0026#39;) s+=1 n=0 if s==100: print() s=0 input_image=x_train_noisy_[i:i+1] target=x_train_[i:i+1] train_step(input_image, target, 1) n+=1 \u0026#39;\u0026#39;\u0026#39;for n, (input_image, target) in train_ds.enumerate(): print(\u0026#39;.\u0026#39;, end=\u0026#39;\u0026#39;) if (n+1) % 100 == 0: print() train_step(input_image, target, epoch)\u0026#39;\u0026#39;\u0026#39; print() # saving (checkpoint) the model every 20 epochs if (epoch + 1) % 3 == 0: !rm -Rf /content/drive/MyDrive/project/training_checkpoints/ckpt-* checkpoint.save(file_prefix=checkpoint_prefix) print (\u0026#39;Time taken for epoch {}is {}sec\\n\u0026#39;.format(epoch + 1, time.time()-start)) checkpoint.save(file_prefix=checkpoint_prefix) Source imageRestorationUsingGans.ipynb  Thanks for reading! ","date":"Sep 6, 2020","img":"https://g00g1y5p4.github.io/posts/imagerestorationusinggans_2/images/home.jpg","permalink":"https://g00g1y5p4.github.io/posts/imagerestorationusinggans_2/","series":[],"tags":["Machine Learning","Deep Learning","Image Processing"],"title":"Image Restoration Using GAN's Part-2"},{"categories":["Game"],"content":"Introduction Hey matesüòä. How are you? Hope you all begin well. Do you know 2048 game? Yeah! Of course, everyone knew that. Ok then, Let me create it on my own ü•≤.\nWhy I\u0026rsquo;m doing this GameBoard? Actually, One of my seniors in my college asked me to do this for their exam. That senior need to submit this GameBoard code which is written in python to get selected for InterView in OffCampus placement. OMG author, Did seniors asks you to code their project? Yes, but some seniors only ü•±. Is it trueüò¨? Ofcourse yes reyü§´. But .. ? Just stop asking questionü§ß?\nLets Code it. Total number of blocks in 2048 is 16 which is looks like 4x4 matrix. First of all we need a empty board to start the game. So we have to create a empty 2D array of length 4 and width also 4. And at starting of the game we also need to generate the board with a value 2 at random place.\nimport random emptyIndexCount = 16 gameBoard = [\t[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0] ] seedIndex = random.randint(1,16) gameBoard[seedIndex_//4][seedIndex_-4*(seedIndex_//4)] = 2 print(gameBoard)  [[0, 0, 0, 0], [0, 0, 0, 0], [0, 2, 0, 0], [0, 0, 0, 0]]  Now our initial step to build gameBoard is successfully done. So Now going to step2, In step 2 user need to give input to gameBoard. Initially let work on left move. So when user gives left Input then what we need to do is remove empty indexes and collect others values in each row and shift them to left. ex: for above array after left shift\n\t[[0, 0, 0, 0], [0, 0, 0, 0], [2, 0, 0, 0], [0, 0, 0, 0]]  After shifting value to left we need to add values if corresponding index holded values are same and move to left by 1 index.\n[[0,2,2,4]] =\u0026gt; [[2,2,4,0]] =\u0026gt; [[4,4,0,0,]]  lets implement this\nuserInput_ = input(\u0026#34;1-up/2-down/3-right/4-left: \u0026#34;) if int(userInput_) == 4: for i in range(4): columnList = gameBoard[i] _backupList = [i for i in columnList if i!=0]+[0,0,0,0] _backupList = _backupList[0:4] print() gameBoard[i] = _backupList for i in range(4): if gameBoard[i][0] == gameBoard[i][1]: gameBoard[i][0] = gameBoard[i][1] + gameBoard[i][0] if gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][1] = gameBoard[i][1] + gameBoard[i][2] gameBoard[i][2] = 0 gameBoard[i][3] = 0 else: gameBoard[i][1] = gameBoard[i][2] gameBoard[i][2] = gameBoard[i][3] gameBoard[i][3] = 0 elif gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][1] = gameBoard[i][1] + gameBoard[i][2] gameBoard[i][2] = gameBoard[i][3] gameBoard[i][3] = 0 elif gameBoard[i][2] == gameBoard[i][3]: gameBoard[i][2] = gameBoard[i][2] + gameBoard[i][3] gameBoard[i][3] = 0 In the above code I have checked that if corresponding values in the resultant array is matched then I add them and replace the result sum with first indexed value.\n[[0,2,4,4]] =\u0026gt; [[2,4,4]]+[[0]] =\u0026gt; [[2,4,4,0]] =\u0026gt; [[2,8,0,0]]  Huh! Irritating arrays right? haha .. yes but very easy. And one more thing Now we have to generate another number in random position in gameBoard. So, first we need to take list of empty indexes and the count of that list. And that random number is between 0 to maxnumber which is existed in gameBoard.\nseedNumber_ = [2,4,8,16,32,64,128,254,512,1024,2048] maxNumber = 0 count_=0 index = [] indexCount = 0 for row in gameBoard: for col in row: if col==0: count_+=1 index.append(indexCount) if col\u0026gt;maxNumber_: maxNumber_ = col indexCount+=1 seedIndex_ = random.choice(index) if maxNumber_==0: randomSeed_ = 2 else: #print(random.choice(seedNumber_[0:seedNumber_.index(maxNumber_)+1])) randomSeed_ = random.choice(seedNumber_[0:seedNumber_.index(maxNumber_)+1]) #\tprint(maxNumber_) gameBoard[seedIndex_//4][seedIndex_-4*(seedIndex_//4)] = randomSeed_ print(gameBoard) This will generate the new random value in any of the empty indexes.\n\t[[0, 0, 0, 0], [0, 0, 2, 0], ==\u0026gt; generated random value at gameBoard[1][2] position [2, 0, 0, 0], [0, 0, 0, 0]]  Okay! Same for the right but the list is reversed in above code\u0026rsquo;s first if condition\nuserInput_ = input(\u0026#34;1-up/2-down/3-right/4-left: \u0026#34;) if int(userInput_) == 3: for i in range(4): columnList = gameBoard[i] _backupList = [i for i in columnList if i!=0]+[0,0,0,0] _backupList = _backupList[0:4][::-1] print() gameBoard[i] = _backupList for i in range(4): if gameBoard[i][3] == gameBoard[i][2]: gameBoard[i][3] = gameBoard[i][3] + gameBoard[i][2] if gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][2] = gameBoard[i][2] + gameBoard[i][1] gameBoard[i][1] = 0 gameBoard[i][0] = 0 else: gameBoard[i][2] = gameBoard[i][1] gameBoard[i][1] = gameBoard[i][0] gameBoard[i][0] = 0 elif gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][2] = gameBoard[i][1] + gameBoard[i][2] gameBoard[i][1] = gameBoard[i][0] gameBoard[i][0] = 0 elif gameBoard[i][1] == gameBoard[i][0]: gameBoard[i][1] = gameBoard[i][1] + gameBoard[i][0] gameBoard[i][0] = 0 This will throw value in gameboard to right side and new random value also generated because of loop.\n\t[[0, 0, 0, 0], [0, 0, 0, 2], [0, 2, 0, 2], ==\u0026gt; generated random value at gameBoard[2][1] position [0, 0, 0, 0]]  We are completed the Left and Right key operation to 2048 gameBoard. Now, we need to perform Up and Down key to the gameBoard. So let us do for UP key first\nWhen Up key presses what happends in gameBoard?\n2048GameBoard.py Hope you guys like and Subscribe to this Utube account üòÇüòÇ.\n Thanks for reading! ","date":"Apr 17, 2020","img":"","permalink":"https://g00g1y5p4.github.io/posts/2048withpython/","series":[],"tags":["",""],"title":"2048 With Python"},{"categories":["Socket"],"content":"Introduction Hey matesüòä. How are you? Hope you all are begin good.By the way, This is my first blog. Did you know my name ü•±? Well, I am Akash.\nWhy we are here? To make some fun? üòï! No, But we\u0026rsquo;re here to learn something new in a funny way. Funny while learningüòí? Yeah!.,\nBy the way, What\u0026rsquo;s the new thing? Guess? ü§îü§î.. How to copy some code from StackOverflow easily using python? üòÖ, No. We are here to learn How to code Simple VideoCall App with python sockets.\nVideoCall App? Android Application ah? No, Just a simple python interpreter application. But useful for future stepüòä.\nAbout project ..! This is a simple VideoCallApp created with python script using sockets and opencv. We are creating a simple TCP protocol using python sockets. We will read Video data from camera or webcam using opencv and Audio from microphone using pyAudio which are under Async function. I\u0026rsquo;m using two open ports to make communication between server and client. One port is for Video Transmission and one port is for Audio Transmission. I think by using two ports there will be no disturbances in socket communication.\nWhy I did this project ? I think it\u0026rsquo;s 2019 September. I was studying Engg-1. Most of my friends are using smartphones and I\u0026rsquo;m using a Jio smartPhone(KeyBoard phone). I was very curious about networking at that time. My friends are doing Videochat with thier friends in smartphone, but I dont have one. One day I got this idea, why don\u0026rsquo;t I made my own VideoCallingApp. I was already knew Image processing and openCV. So I can manage VideoStream and Audio too. That\u0026rsquo;s why I decided to do this App.\nLibraries I\u0026rsquo;m using python to create this Application. Here are the list of libraries I had used to build this application\nimport cv2, numpy as np import pyaudio import socket import pickle import sys,time  Socket is python Library used to connect two nodes on a network to communicate with each other. One socket(node) listens on a particular port at an IP, while the other socket reaches out to the other to form a connection. The server forms the listener socket while the client reaches out to the server. pyAudio is used to read and write data from microphone. opencv is also used to read data from webcam  How to create socket and Bind connection ## serverside simpleSocket= socket.socket(socket.AF_INET,socket.SOCK_STREAM) simpleSocket.bind((\u0026#39;IP\u0026#39;,\u0026#39;PORT\u0026#39;)) simpleSocket.listen(5) conn , addr = simpleSocket.accept() conn.send(b\u0026#34;123 -- sent by server\u0026#34;) ## clientSide simpleSocket_ = socket.socket(socket.AF_INET,socket.SOCK_STREAM) simpleSocket_.connect((\u0026#39;IP\u0026#39;,\u0026#39;PORT\u0026#39;)) data = simpleSocket.recv(21) print(str(data)) b'123 -- sent by server'  This is the example of simple socket connection between a server and client. AF_INET is the Internet address family for IPv4. SOCK_STREAM is the socket type for TCP, the protocol that will be used to transport our messages in the network.\nRead data from webCam and MicroPhone ## reading data from webCamera video = cv2.VideoCapture(0) _, frame = video.read() print(frame) video.release() cv2.destroyAllWindows() ## Reading data from microPhone chunk = 1024 FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 44100 p = pyaudio.PyAudio() #initializing microphone stream = p.open(format = FORMAT, channels = CHANNELS, rate = RATE, input = True, frames_per_buffer = chunk ) stream.read(chunk) print(stream) prints ImageData in RGB array format prints AudioData in 1D array format  Sending and Receiving Data from socket ##################### ImageData ################ ## serverSide length = 0 while length\u0026lt;921764: pac=simpleSocket.recv(9999999) length+=len(pac) data+=pac receivedImage=pickle.loads(data) ## clientSide img_data=np.array(frame) data=pickle.dumps(img_data) simpleSocket_.send(data) #################### AudioData ################# #serverSide audioData = simpleSocket.recv(chunk) audioData = pickle.loads(audioData) stream.write(audioData) #clientSide data = stream.read(chunk) if data: simpleSocket_.send(data) receivedImage =\u0026gt; imageData is sent from client's socket and received to server's socket stream.write =\u0026gt; audio data is read from microphone and sent from client's socket and received to server's socket and writes data on stream  That\u0026rsquo;s it! We partially completed our main goal of the project. We received Image data and mircoPhone data from client to server for one time.\nBut the thing is, If we run a loop to send and retreive data from client to server its like first it sends image data and then microphone data. Again repeat..!. But we want Async data retreival from client to run video and audio simultaneously. So we use Threads to do this.\nAsync ##################### ClientSide ################# from threading import Thread def recordAudio(): time.sleep(5) while True: data = stream.read(chunk) if data: simpleSocket_.sendall(data) def sendVideo(): global conn,video while 1: try: _,frame=video.read() img_data=np.array(frame) data=pickle.dumps(img_data) conn.send(data) except KeyboardInterrupt: video.release() sys.exit() sendVideo = serverVideo.sendVideo sendAudio = serverAudio.recordAudio sendV = Thread(target = sendVideo) sendA = Thread(target = sendAudio) recvA.start() sendA.start() ####################### serverSide ################# from threading import Thread def rcvAudio(): while True: audioData = simpleSocket.recv(chunk) audioData = pickle.loads(audioData) stream.write(audioData) def recvVideo(): global conn while 1: try: data=b\u0026#34;\u0026#34; length=0 while length\u0026lt;921764: pac=conn.recv(9999999) length+=len(pac) data+=pac if data: imgData=pickle.loads(data) cv2.imshow(\u0026#34;ClientData\u0026#34;,imgData) key=cv2.waitKey(1) if key == 27: break except KeyboardInterrupt: sys.exit() recvVideo = serverVideo.recvVideo recvAudio = serverAudio.rcvAudio recvA = Thread(target = recvAudio) recvV = Thread(target = recvVideo) recvV.start() sendV.start() Now, server gets continious data of Clients Video and Audio.  Now, We just have to write receieve functions for Image and Audio data in client side and sending function in serverside.\nsimpleVideoCallApp Hope you guys like and Subscribe to this Utube account üòÇüòÇ.\n Thanks for reading! ","date":"Jan 20, 2020","img":"https://g00g1y5p4.github.io/posts/simplevideocallapp/images/1.png","permalink":"https://g00g1y5p4.github.io/posts/simplevideocallapp/","series":[],"tags":["ImageProcessing","openCV"],"title":"A Simple Video Call App With Python Sockets"}]